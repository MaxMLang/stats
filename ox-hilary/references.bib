
@article{meng_simulating_1996,
	title = {Simulating {Ratios} of {Normalizing} {Constants} {Via} a {Simple} {Identity}: {A} {Theoretical} {Exploration}},
	volume = {6},
	issn = {1017-0405},
	shorttitle = {Simulating {Ratios} of {Normalizing} {Constants} {Via} a {Simple} {Identity}},
	url = {https://www.jstor.org/stable/24306045},
	abstract = {Let pi(w),i = 1,2, be two densities with common support where each density is known up to a normalizing constant: pi(w) = qi(w)/ci. We have draws from each density (e.g., via Markov chain Monte Carlo), and we want to use these draws to simulate the ratio of the normalizing constants, c1/c2. Such a computational problem is often encountered in likelihood and Bayesian inference, and arises in fields such as physics and genetics. Many methods proposed in statistical and other literature (e.g., computational physics) for dealing with this problem are based on various special cases of the following simple identity: \${\textbackslash}frac\{c\_\{1\}\}\{c\_\{2\}\} = {\textbackslash}frac\{E\_\{2\}[q\_\{1\}(w){\textbackslash}alpha (w)]\}\{E\_\{1\}[q\_\{2\}(w){\textbackslash}alpha (w)]\}\$ Here Ei denotes the expectation with respect to pi (i = 1,2), and α is an arbitrary function such that the denominator is non-zero. A main purpose of this paper is to provide a theoretical study of the usefulness of this identity, with focus on (asymptotically) optimal and practical choices of α. Using a simple but informative example, we demonstrate that with sensible (not necessarily optimal) choices of α, we can reduce the simulation error by orders of magnitude when compared to the conventional importance sampling method, which corresponds to α = 1/q2. We also introduce several generalizations of this identity for handling more complicated settings (e.g., estimating several ratios simultaneously) and pose several open problems that appear to have practical as well as theoretical value. Furthermore, we discuss related theoretical and empirical work.},
	number = {4},
	urldate = {2024-01-31},
	journal = {Statistica Sinica},
	author = {Meng, Xiao-Li and Wong, Wing Hung},
	year = {1996},
	note = {Publisher: Institute of Statistical Science, Academia Sinica},
	pages = {831--860},
}

@article{meng_simulating_1996-1,
	title = {Simulating {Ratios} of {Normalizing} {Constants} {Via} a {Simple} {Identity}: {A} {Theoretical} {Exploration}},
	volume = {6},
	issn = {1017-0405},
	shorttitle = {Simulating {Ratios} of {Normalizing} {Constants} {Via} a {Simple} {Identity}},
	url = {https://www.jstor.org/stable/24306045},
	abstract = {Let pi(w),i = 1,2, be two densities with common support where each density is known up to a normalizing constant: pi(w) = qi(w)/ci. We have draws from each density (e.g., via Markov chain Monte Carlo), and we want to use these draws to simulate the ratio of the normalizing constants, c1/c2. Such a computational problem is often encountered in likelihood and Bayesian inference, and arises in fields such as physics and genetics. Many methods proposed in statistical and other literature (e.g., computational physics) for dealing with this problem are based on various special cases of the following simple identity: \${\textbackslash}frac\{c\_\{1\}\}\{c\_\{2\}\} = {\textbackslash}frac\{E\_\{2\}[q\_\{1\}(w){\textbackslash}alpha (w)]\}\{E\_\{1\}[q\_\{2\}(w){\textbackslash}alpha (w)]\}\$ Here Ei denotes the expectation with respect to pi (i = 1,2), and α is an arbitrary function such that the denominator is non-zero. A main purpose of this paper is to provide a theoretical study of the usefulness of this identity, with focus on (asymptotically) optimal and practical choices of α. Using a simple but informative example, we demonstrate that with sensible (not necessarily optimal) choices of α, we can reduce the simulation error by orders of magnitude when compared to the conventional importance sampling method, which corresponds to α = 1/q2. We also introduce several generalizations of this identity for handling more complicated settings (e.g., estimating several ratios simultaneously) and pose several open problems that appear to have practical as well as theoretical value. Furthermore, we discuss related theoretical and empirical work.},
	number = {4},
	urldate = {2024-01-31},
	journal = {Statistica Sinica},
	author = {Meng, Xiao-Li and Wong, Wing Hung},
	year = {1996},
	note = {Publisher: Institute of Statistical Science, Academia Sinica},
	pages = {831--860},
}

@article{dawid_decision-theoretic_2021,
	title = {Decision-theoretic foundations for statistical causality},
	volume = {9},
	issn = {2193-3685},
	url = {https://www.degruyter.com/document/doi/10.1515/jci-2020-0008/html},
	doi = {10.1515/jci-2020-0008},
	abstract = {We develop a mathematical and interpretative foundation for the enterprise of decision-theoretic (DT) statistical causality, which is a straightforward way of representing and addressing causal questions. DT reframes causal inference as “assisted decision-making” and aims to understand when, and how, I can make use of external data, typically observational, to help me solve a decision problem by taking advantage of assumed relationships between the data and my problem. The relationships embodied in any representation of a causal problem require deeper justiﬁcation, which is necessarily context-dependent. Here we clarify the considerations needed to support applications of the DT methodology. Exchangeability considerations are used to structure the required relationships, and a distinction drawn between intention to treat and intervention to treat forms the basis for the enabling condition of “ignorability.” We also show how the DT perspective uniﬁes and sheds light on other popular formalisations of statistical causality, including potential responses and directed acyclic graphs.},
	language = {en},
	number = {1},
	urldate = {2024-01-26},
	journal = {Journal of Causal Inference},
	author = {Dawid, Philip},
	month = may,
	year = {2021},
	pages = {39--77},
}

@article{vanderweele_sufficient_2009,
	title = {Sufficient {Cause} {Interactions} and {Statistical} {Interactions}},
	volume = {20},
	issn = {1044-3983},
	url = {https://journals.lww.com/00001648-200901000-00004},
	doi = {10.1097/EDE.0b013e31818f69e7},
	abstract = {When the outcome and all exposures of interest are binary it is sometimes possible to draw conclusions from empirical data about mechanistic interactions in the sufﬁcient cause sense. Empirical conditions are given for sufﬁcient cause interactions and these conditions are compared with and contrasted to interaction coefﬁcients in linear, log-linear and logistic regression models. Conditions that sufﬁce to allow for the interpretation of statistical interactions as sufﬁcient cause interactions are derived. Discussion is presented concerning the implications of the inclusion of confounding variables in the model.},
	language = {en},
	number = {1},
	urldate = {2024-01-26},
	journal = {Epidemiology},
	author = {VanderWeele, Tyler J.},
	month = jan,
	year = {2009},
	pages = {6--13},
}

@article{vanderweele_sufficient_2009-1,
	title = {Sufficient {Cause} {Interactions} and {Statistical} {Interactions}},
	volume = {20},
	issn = {1044-3983},
	url = {https://journals.lww.com/00001648-200901000-00004},
	doi = {10.1097/EDE.0b013e31818f69e7},
	abstract = {When the outcome and all exposures of interest are binary it is sometimes possible to draw conclusions from empirical data about mechanistic interactions in the sufﬁcient cause sense. Empirical conditions are given for sufﬁcient cause interactions and these conditions are compared with and contrasted to interaction coefﬁcients in linear, log-linear and logistic regression models. Conditions that sufﬁce to allow for the interpretation of statistical interactions as sufﬁcient cause interactions are derived. Discussion is presented concerning the implications of the inclusion of confounding variables in the model.},
	language = {en},
	number = {1},
	urldate = {2024-01-26},
	journal = {Epidemiology},
	author = {VanderWeele, Tyler J.},
	month = jan,
	year = {2009},
	pages = {6--13},
}

@article{dawid_decision-theoretic_2021-1,
	title = {Decision-theoretic foundations for statistical causality},
	volume = {9},
	issn = {2193-3685},
	url = {https://www.degruyter.com/document/doi/10.1515/jci-2020-0008/html},
	doi = {10.1515/jci-2020-0008},
	abstract = {We develop a mathematical and interpretative foundation for the enterprise of decision-theoretic (DT) statistical causality, which is a straightforward way of representing and addressing causal questions. DT reframes causal inference as “assisted decision-making” and aims to understand when, and how, I can make use of external data, typically observational, to help me solve a decision problem by taking advantage of assumed relationships between the data and my problem. The relationships embodied in any representation of a causal problem require deeper justiﬁcation, which is necessarily context-dependent. Here we clarify the considerations needed to support applications of the DT methodology. Exchangeability considerations are used to structure the required relationships, and a distinction drawn between intention to treat and intervention to treat forms the basis for the enabling condition of “ignorability.” We also show how the DT perspective uniﬁes and sheds light on other popular formalisations of statistical causality, including potential responses and directed acyclic graphs.},
	language = {en},
	number = {1},
	urldate = {2024-01-26},
	journal = {Journal of Causal Inference},
	author = {Dawid, Philip},
	month = may,
	year = {2021},
	pages = {39--77},
}

@article{leemis_univariate_2008,
	title = {Univariate {Distribution} {Relationships}},
	volume = {62},
	issn = {0003-1305, 1537-2731},
	url = {http://www.tandfonline.com/doi/abs/10.1198/000313008X270448},
	doi = {10.1198/000313008X270448},
	language = {en},
	number = {1},
	urldate = {2024-01-24},
	journal = {The American Statistician},
	author = {Leemis, Lawrence M and McQueston, Jacquelyn T},
	month = feb,
	year = {2008},
	pages = {45--53},
}

@article{zhao_framework_2018,
	title = {A {Framework} of {Rebalancing} {Imbalanced} {Healthcare} {Data} for {Rare} {Events}’ {Classification}: {A} {Case} of {Look}-{Alike} {Sound}-{Alike} {Mix}-{Up} {Incident} {Detection}},
	volume = {2018},
	issn = {2040-2295, 2040-2309},
	shorttitle = {A {Framework} of {Rebalancing} {Imbalanced} {Healthcare} {Data} for {Rare} {Events}’ {Classification}},
	url = {https://www.hindawi.com/journals/jhe/2018/6275435/},
	doi = {10.1155/2018/6275435},
	abstract = {Identifying rare but significant healthcare events in massive unstructured datasets has become a common task in healthcare data analytics. However, imbalanced class distribution in many practical datasets greatly hampers the detection of rare events, as most classification methods implicitly assume an equal occurrence of classes and are designed to maximize the overall classification accuracy. In this study, we develop a framework for learning healthcare data with imbalanced distribution via incorporating different rebalancing strategies. The evaluation results showed that the developed framework can significantly improve the detection accuracy of medical incidents due to look-alike sound-alike (LASA) mix-ups. Specifically, logistic regression combined with the synthetic minority oversampling technique (SMOTE) produces the best detection results, with a significant 45.3\% increase in recall (
              
                r
                e
                c
                a
                l
                l
                =
                75.7
                \%
              
              ) compared with pure logistic regression (
              
                r
                e
                c
                a
                l
                l
                =
                52.1
                \%
              
              ).},
	language = {en},
	urldate = {2024-01-19},
	journal = {Journal of Healthcare Engineering},
	author = {Zhao, Yang and Wong, Zoie Shui-Yee and Tsui, Kwok Leung},
	year = {2018},
	pages = {1--11},
}

@article{keilis-borok_prediction_nodate,
	title = {Prediction of {Extreme} {Events} in {Socio}-economic {Systems}},
	language = {en},
	author = {Keilis-Borok, V and Soloviev, A},
}

@misc{bengio_representation_2014,
	title = {Representation {Learning}: {A} {Review} and {New} {Perspectives}},
	shorttitle = {Representation {Learning}},
	url = {http://arxiv.org/abs/1206.5538},
	abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.},
	language = {en},
	urldate = {2024-01-19},
	publisher = {arXiv},
	author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
	month = apr,
	year = {2014},
	note = {arXiv:1206.5538 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{himeur_artificial_2021,
	title = {Artificial intelligence based anomaly detection of energy consumption in buildings: {A} review, current trends and new perspectives},
	volume = {287},
	issn = {03062619},
	shorttitle = {Artificial intelligence based anomaly detection of energy consumption in buildings},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0306261921001409},
	doi = {10.1016/j.apenergy.2021.116601},
	abstract = {Enormous amounts of data are being produced everyday by sub-meters and smart sensors installed in residential buildings. If leveraged properly, that data could assist end-users, energy producers and utility companies in detecting anomalous power consumption and understanding the causes of each anomaly. Therefore, anomaly detection could stop a minor problem becoming overwhelming. Moreover, it will aid in better decision-making to reduce wasted energy and promote sustainable and energy efficient behavior. In this regard, this paper is an in-depth review of existing anomaly detection frameworks for building energy consumption based on artificial intelligence. Specifically, an extensive survey is presented, in which a comprehensive taxonomy is introduced to classify existing algorithms based on different modules and parameters adopted, such as machine learning algorithms, feature extraction approaches, anomaly detection levels, computing platforms and application scenarios. To the best of the authors’ knowledge, this is the first review article that discusses anomaly detection in building energy consumption. Moving forward, important findings along with domain-specific problems, difficulties and challenges that remain unresolved are thoroughly discussed, including the absence of: (i) precise definitions of anomalous power consumption, (ii) annotated datasets, (iii) unified metrics to assess the performance of existing solutions, (iv) platforms for reproducibility and (v) privacy-preservation. Following, insights about current research trends are discussed to widen the applications and effectiveness of the anomaly detection technology before deriving future directions attracting significant attention. This article serves as a comprehensive reference to understand the current technological progress in anomaly detection of energy consumption based on artificial intelligence.},
	language = {en},
	urldate = {2024-01-19},
	journal = {Applied Energy},
	author = {Himeur, Yassine and Ghanem, Khalida and Alsalemi, Abdullah and Bensaali, Faycal and Amira, Abbes},
	month = apr,
	year = {2021},
	pages = {116601},
}

@article{efron_empirical_nodate,
	title = {Empirical {Bayes}: {Concepts} and {Methods}},
	abstract = {Empirical Bayes methods originated in the work of Robbins, Good and Toulmin, Stein and others in the 1950s. This overview of the main ideas focuses on some empirical Bayes success stories: the missing species problem, James–Stein shrinkage estimators, and Bayesian false discovery rates. Diﬀerent modeling strategies are reviewed, as well as the dual frequentist/Bayesian nature of the methods. Technical issues are kept to a minimum, at the expense of avoiding most theoretical developments. The ﬁnal section discusses the problem of relevance–how to decide which “other” observations apply to inference about a particular case of interest.},
	language = {en},
	author = {Efron, Bradley},
}

@article{wagenmakers_history_2023,
	title = {History and nature of the {Jeffreys}–{Lindley} paradox},
	volume = {77},
	issn = {0003-9519, 1432-0657},
	url = {https://link.springer.com/10.1007/s00407-022-00298-3},
	doi = {10.1007/s00407-022-00298-3},
	abstract = {Abstract
            
              The Jeffreys–Lindley paradox exposes a rift between Bayesian and frequentist hypothesis testing that strikes at the heart of statistical inference. Contrary to what most current literature suggests, the paradox was central to the Bayesian testing methodology developed by Sir Harold Jeffreys in the late 1930s. Jeffreys showed that the evidence for a point-null hypothesis
              
                
                  \$\$\{{\textbackslash}mathcal \{H\}\}\_0\$\$
                  
                    
                      H
                      0
                    
                  
                
              
              scales with
              
                
                  \$\${\textbackslash}sqrt\{n\}\$\$
                  
                    
                      n
                    
                  
                
              
              and repeatedly argued that it would, therefore, be mistaken to set a threshold for rejecting
              
                
                  \$\$\{{\textbackslash}mathcal \{H\}\}\_0\$\$
                  
                    
                      H
                      0
                    
                  
                
              
              at a constant multiple of the standard error. Here, we summarize Jeffreys’s early work on the paradox and clarify his reasons for including the
              
                
                  \$\${\textbackslash}sqrt\{n\}\$\$
                  
                    
                      n
                    
                  
                
              
              term. The prior distribution is seen to play a crucial role; by implicitly correcting for selection, small parameter values are identified as relatively surprising under
              
                
                  \$\$\{{\textbackslash}mathcal \{H\}\}\_1\$\$
                  
                    
                      H
                      1
                    
                  
                
              
              . We highlight the general nature of the paradox by presenting both a fully frequentist and a fully Bayesian version. We also demonstrate that the paradox does not depend on assigning prior mass to a point hypothesis, as is commonly believed.},
	language = {en},
	number = {1},
	urldate = {2024-01-19},
	journal = {Archive for History of Exact Sciences},
	author = {Wagenmakers, Eric-Jan and Ly, Alexander},
	month = jan,
	year = {2023},
	pages = {25--72},
}

@book{bolstad_introduction_2007,
	address = {Hoboken, New Jersey},
	edition = {Second edition},
	title = {Introduction to {Bayesian} statistics},
	isbn = {978-0-470-14115-1},
	language = {en},
	publisher = {Wiley \& Sons, Inc., Publication},
	author = {Bolstad, William M.},
	year = {2007},
}

@article{robert_harold_2009,
	title = {Harold {Jeffreys}'s {Theory} of {Probability} {Revisited}},
	volume = {24},
	issn = {0883-4237},
	url = {http://arxiv.org/abs/0804.3173},
	doi = {10.1214/09-STS284},
	abstract = {Published exactly seventy years ago, Jeﬀreys’s Theory of Probability (1939) has had a unique impact on the Bayesian community and is now considered to be one of the main classics in Bayesian Statistics as well as the initiator of the objective Bayes school. In particular, its advances on the derivation of noninformative priors as well as on the scaling of Bayes factors have had a lasting impact on the ﬁeld. However, the book reﬂects the characteristics of the time, especially in terms of mathematical rigor. In this paper we point out the fundamental aspects of this reference work, especially the thorough coverage of testing problems and the construction of both estimation and testing noninformative priors based on functional divergences. Our major aim here is to help modern readers in navigating in this diﬃcult text and in concentrating on passages that are still relevant today.},
	language = {en},
	number = {2},
	urldate = {2024-01-19},
	journal = {Statistical Science},
	author = {Robert, Christian P. and Chopin, Nicolas and Rousseau, Judith},
	month = may,
	year = {2009},
	note = {arXiv:0804.3173 [math, stat]},
	keywords = {Mathematics - History and Overview, Mathematics - Statistics Theory},
}

@book{parr_active_2022,
	address = {Cambridge, Massachusetts},
	title = {Active inference: the free energy principle in mind, brain, and behavior},
	isbn = {978-0-262-04535-3},
	shorttitle = {Active inference},
	abstract = {"A much-needed synthesis of active inference, a theory of mind that addresses cognition, behavior, intelligence, \& mental disorders and which can be extended to explain behavior in all living systems"--},
	language = {en},
	publisher = {The MIT Press},
	author = {Parr, Thomas and Pezzulo, Giovanni and Friston, K. J.},
	year = {2022},
	keywords = {Bayesian statistical decision theory, Human behavior models, Inference, Knowledge, Theory of, Neurobiology, Perception},
}

@article{hofner_model-based_2014,
	title = {Model-based boosting in {R}: a hands-on tutorial using the {R} package mboost},
	volume = {29},
	issn = {0943-4062, 1613-9658},
	shorttitle = {Model-based boosting in {R}},
	url = {http://link.springer.com/10.1007/s00180-012-0382-5},
	doi = {10.1007/s00180-012-0382-5},
	language = {en},
	number = {1-2},
	urldate = {2024-01-19},
	journal = {Computational Statistics},
	author = {Hofner, Benjamin and Mayr, Andreas and Robinzonov, Nikolay and Schmid, Matthias},
	month = feb,
	year = {2014},
	pages = {3--35},
}

@article{craven_smoothing_nodate,
	title = {Smoothing noisy data with spline functions},
	abstract = {Smoothing splines are well known to provide nice curves which smooth discrete, noisy data. We obtain a practical, effective method for estimating the optimum amount of smoothing from the data. Derivatives can be estimated from the data by differentiating the resulting (nearly) optimally smoothed spline.},
	language = {en},
	author = {Craven, Peter and Wahba, Grace},
}

@article{wood_generalized_nodate,
	title = {Generalized {Additive} {Models}: an introduction with {R}},
	language = {en},
	author = {Wood, Simon N},
}

@article{marra_practical_2011,
	title = {Practical variable selection for generalized additive models},
	volume = {55},
	issn = {01679473},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167947311000491},
	doi = {10.1016/j.csda.2011.02.004},
	abstract = {The problem of variable selection within the class of generalized additive models, when there are many covariates to choose from but the number of predictors is still somewhat smaller than the number of observations, is considered. Two very simple but effective shrinkage methods and an extension of the nonnegative garrote estimator are introduced. The proposals avoid having to use nonparametric testing methods for which there is no general reliable distributional theory. Moreover, component selection is carried out in one single step as opposed to many selection procedures which involve an exhaustive search of all possible models. The empirical performance of the proposed methods is compared to that of some available techniques via an extensive simulation study. The results show under which conditions one method can be preferred over another, hence providing applied researchers with some practical guidelines. The procedures are also illustrated analysing data on plasma beta-carotene levels from a cross-sectional study conducted in the United States.},
	language = {en},
	number = {7},
	urldate = {2024-01-19},
	journal = {Computational Statistics \& Data Analysis},
	author = {Marra, Giampiero and Wood, Simon N.},
	month = jul,
	year = {2011},
	pages = {2372--2387},
}

@article{stasinopoulos_gamlss_nodate,
	title = {{GAMLSS} practicals for the {Graz} short course},
	language = {en},
	author = {Stasinopoulos, Mikis},
}

@incollection{freedman_what_2009,
	edition = {1},
	title = {What is the {Chance} of an {Earthquake}?},
	isbn = {978-0-521-19500-3 978-0-521-12390-7 978-0-511-81587-4},
	url = {https://www.cambridge.org/core/product/identifier/CBO9780511815874A019/type/book_part},
	language = {en},
	urldate = {2024-01-19},
	booktitle = {Statistical {Models} and {Causal} {Inference}},
	publisher = {Cambridge University Press},
	author = {Stark, Philip B.},
	editor = {Collier, David and Sekhon, Jasjeet S. and Stark, Philip B.},
	collaborator = {Freedman, David A.},
	month = nov,
	year = {2009},
	doi = {10.1017/CBO9780511815874.010},
	pages = {115--130},
}

@article{paskin_3_nodate,
	title = {3. {The} {Junction} {Tree} {Algorithms}},
	language = {en},
	author = {Paskin, Mark},
}

@article{sarrafzadeh_department_1990,
	title = {Department of electrical engineering and computer science},
	volume = {20},
	issn = {0163-5743},
	url = {https://dl.acm.org/doi/10.1145/378886.380416},
	doi = {10.1145/378886.380416},
	language = {en},
	number = {1},
	urldate = {2024-01-19},
	journal = {ACM SIGDA Newsletter},
	author = {Sarrafzadeh, M.},
	month = jun,
	year = {1990},
	pages = {91},
}

@article{scribe_3_nodate,
	title = {3: {Directed} {Graphical} {Models} ({Bayesian} {Networks})},
	language = {en},
	journal = {Bayesian Networks},
	author = {Scribe, Eric P Xing and Pokharel, Suman and Yang, Wendy and Yan, Donghui and Tang, Jingjing and Sun, Wenhuan},
}

@article{xing_2_nodate,
	title = {2: {Representation} of undirected graphical models},
	language = {en},
	author = {Xing, Eric P and Wortwein, T and Lee, H and Rawal, V and Ferdosi, M},
}

@article{xing_10_nodate,
	title = {10 : {Gaussian} graphical models and {Ising} models: modeling networks},
	language = {en},
	author = {Xing, Eric P and Lee, Min Hyung and Xia, Yan},
}

@article{lomax_estimating_2016,
	title = {Estimating {Population} {Attribute} {Values} in a {Table}: “{Get} {Me} {Started} in” {Iterative} {Proportional} {Fitting}},
	volume = {68},
	issn = {0033-0124, 1467-9272},
	shorttitle = {Estimating {Population} {Attribute} {Values} in a {Table}},
	url = {https://www.tandfonline.com/doi/full/10.1080/00330124.2015.1099449},
	doi = {10.1080/00330124.2015.1099449},
	language = {en},
	number = {3},
	urldate = {2024-01-19},
	journal = {The Professional Geographer},
	author = {Lomax, Nik and Norman, Paul},
	month = jul,
	year = {2016},
	pages = {451--461},
}

@misc{zheng_dags_2018,
	title = {{DAGs} with {NO} {TEARS}: {Continuous} {Optimization} for {Structure} {Learning}},
	shorttitle = {{DAGs} with {NO} {TEARS}},
	url = {http://arxiv.org/abs/1803.01422},
	abstract = {Estimating the structure of directed acyclic graphs (DAGs, also known as Bayesian networks) is a challenging problem since the search space of DAGs is combinatorial and scales superexponentially with the number of nodes. Existing approaches rely on various local heuristics for enforcing the acyclicity constraint. In this paper, we introduce a fundamentally diﬀerent strategy: We formulate the structure learning problem as a purely continuous optimization problem over real matrices that avoids this combinatorial constraint entirely. This is achieved by a novel characterization of acyclicity that is not only smooth but also exact. The resulting problem can be eﬃciently solved by standard numerical algorithms, which also makes implementation eﬀortless. The proposed method outperforms existing ones, without imposing any structural assumptions on the graph such as bounded treewidth or in-degree. Code implementing the proposed algorithm is open-source and publicly available at https://github.com/xunzheng/notears.},
	language = {en},
	urldate = {2024-01-19},
	publisher = {arXiv},
	author = {Zheng, Xun and Aragam, Bryon and Ravikumar, Pradeep and Xing, Eric P.},
	month = nov,
	year = {2018},
	note = {arXiv:1803.01422 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
}

@book{koller_probabilistic_2009,
	address = {Cambridge, MA},
	series = {Adaptive computation and machine learning},
	title = {Probabilistic graphical models: principles and techniques},
	isbn = {978-0-262-01319-2},
	shorttitle = {Probabilistic graphical models},
	language = {en},
	publisher = {MIT Press},
	author = {Koller, Daphne and Friedman, Nir},
	year = {2009},
	keywords = {Bayesian statistical decision theory, Graphic methods, Graphical modeling (Statistics)},
}

@article{stensrud_limitations_2019,
	title = {Limitations of hazard ratios in clinical trials},
	volume = {40},
	issn = {0195-668X, 1522-9645},
	url = {https://academic.oup.com/eurheartj/article/40/17/1378/5219649},
	doi = {10.1093/eurheartj/ehy770},
	abstract = {Abstract},
	language = {en},
	number = {17},
	urldate = {2024-01-19},
	journal = {European Heart Journal},
	author = {Stensrud, Mats J and Aalen, John M and Aalen, Odd O and Valberg, Morten},
	month = may,
	year = {2019},
	pages = {1378--1383},
}

@article{breheny_likelihood_nodate,
	title = {Likelihood construction},
	language = {en},
	author = {Breheny, Patrick},
}

@article{schmid_wahrscheinlichkeitstheorie_nodate,
	title = {Wahrscheinlichkeitstheorie und {Inferenz} {I}},
	language = {de},
	author = {Schmid, Volker},
}

@article{iyer_lecture_nodate,
	title = {{LECTURE} {NOTES} {ON} {MEASURE} {THEORY} {FALL} 2022},
	language = {en},
	author = {Iyer, Gautam},
}

@article{gupta_measure_nodate,
	title = {A {Measure} {Theory} {Tutorial} ({Measure} {Theory} for {Dummies})},
	abstract = {This tutorial is an informal introduction to measure theory for people who are interested in reading papers that use measure theory. The tutorial assumes one has had at least a year of college-level calculus, some graduate level exposure to random processes, and familiarity with terms like “closed” and “open.” The focus is on the terms and ideas relevant to applied probability and information theory. There are no proofs and no exercises.},
	language = {en},
	author = {Gupta, Maya R},
}

@book{hable_einfuhrung_2015,
	address = {Berlin, Heidelberg},
	series = {Springer-{Lehrbuch}},
	title = {Einführung in die {Stochastik}: {Ein} {Begleitbuch} zur {Vorlesung}},
	isbn = {978-3-662-43497-0 978-3-662-43498-7},
	shorttitle = {Einführung in die {Stochastik}},
	url = {https://link.springer.com/10.1007/978-3-662-43498-7},
	language = {de},
	urldate = {2024-01-19},
	publisher = {Springer Berlin Heidelberg},
	author = {Hable, Robert},
	year = {2015},
	doi = {10.1007/978-3-662-43498-7},
}

@article{hull_random_nodate,
	title = {Random {Number} {Generators}},
	language = {en},
	author = {Hull, T E and Dobell, A R},
}

@book{kauermann_stichproben_2011,
	address = {Berlin, Heidelberg},
	series = {Springer-{Lehrbuch}},
	title = {Stichproben: {Methoden} und praktische {Umsetzung} mit {R}},
	isbn = {978-3-642-12317-7 978-3-642-12318-4},
	shorttitle = {Stichproben},
	url = {https://link.springer.com/10.1007/978-3-642-12318-4},
	language = {de},
	urldate = {2024-01-19},
	publisher = {Springer Berlin Heidelberg},
	author = {Kauermann, Göran and Küchenhoff, Helmut},
	year = {2011},
	doi = {10.1007/978-3-642-12318-4},
}

@book{nikulin_cox_2016,
	address = {Berlin, Heidelberg},
	series = {{SpringerBriefs} in {Statistics}},
	title = {The {Cox} {Model} and {Its} {Applications}},
	isbn = {978-3-662-49331-1 978-3-662-49332-8},
	url = {http://link.springer.com/10.1007/978-3-662-49332-8},
	language = {en},
	urldate = {2024-01-19},
	publisher = {Springer Berlin Heidelberg},
	author = {Nikulin, Mikhail and Wu, Hong-Dar Isaac},
	year = {2016},
	doi = {10.1007/978-3-662-49332-8},
}

@article{cox_regression_2023,
	title = {Regression {Models} and {Life}-{Tables}},
	abstract = {The analysis of censored failure times is considered. It is assumed that on each individual are available values of one or more explanatory variables. The hazard function (age-specific failure rate) is taken to be a function of the explanatory variables and unknown regression coefficients multiplied by an arbitrary and unknown function of time. A conditional likelihood is obtained, leading to inferences about the unknown regression coefficients. Some generalizations are outlined.},
	language = {en},
	author = {Cox, D R},
	year = {2023},
}

@article{schwartz_lecture_nodate,
	title = {Lecture 8: {Fourier} transforms},
	language = {en},
	author = {Schwartz, Matthew},
}

@article{philip_vorlesung_nodate,
	title = {Vorlesung {Analysis} {II} fu¨r {Statistiker}},
	language = {de},
	author = {Philip, P and Feistl, M},
}

@article{philip_vorlesung_nodate-1,
	title = {Vorlesung {Analysis} fu¨r {Informatiker} \& {Statistiker}},
	language = {de},
	author = {Philip, P},
}

@book{fahrmeir_statistik_2016,
	address = {Berlin, Heidelberg},
	series = {Springer-{Lehrbuch}},
	title = {Statistik},
	isbn = {978-3-662-50371-3 978-3-662-50372-0},
	url = {http://link.springer.com/10.1007/978-3-662-50372-0},
	language = {de},
	urldate = {2024-01-19},
	publisher = {Springer Berlin Heidelberg},
	author = {Fahrmeir, Ludwig and Heumann, Christian and Künstler, Rita and Pigeot, Iris and Tutz, Gerhard},
	year = {2016},
	doi = {10.1007/978-3-662-50372-0},
}

@article{faraway_linear_nodate,
	title = {Linear {Models} with {R}},
	language = {en},
	author = {Faraway, Julian J},
}

@book{james_introduction_2013,
	address = {New York, NY},
	series = {Springer {Texts} in {Statistics}},
	title = {An {Introduction} to {Statistical} {Learning}},
	volume = {103},
	isbn = {978-1-4614-7137-0 978-1-4614-7138-7},
	url = {http://link.springer.com/10.1007/978-1-4614-7138-7},
	language = {en},
	urldate = {2024-01-19},
	publisher = {Springer New York},
	author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
	year = {2013},
	doi = {10.1007/978-1-4614-7138-7},
}

@book{fahrmeir_regression_2007,
	address = {Berlin Heidelberg},
	series = {Statistik und ihre {Anwendungen}},
	title = {Regression: {Modelle}, {Methoden} und {Anwendungen}},
	isbn = {978-3-540-33932-8},
	shorttitle = {Regression},
	language = {de},
	publisher = {Springer},
	author = {Fahrmeir, Ludwig and Kneib, Thomas and Lang, Stefan},
	year = {2007},
}

@article{tibshirani_valerie_nodate,
	title = {Valerie and {Patrick} {Hastie}},
	language = {en},
	author = {Tibshirani, Sami and Friedman, Harry},
}

@book{briggs_uncertainty_2016,
	address = {Cham},
	title = {Uncertainty},
	isbn = {978-3-319-39755-9 978-3-319-39756-6},
	url = {http://link.springer.com/10.1007/978-3-319-39756-6},
	language = {en},
	urldate = {2024-01-19},
	publisher = {Springer International Publishing},
	author = {Briggs, William},
	year = {2016},
	doi = {10.1007/978-3-319-39756-6},
}

@article{yung_explaining_nodate,
	title = {Explaining the {Stein} {Paradox}},
	abstract = {This report oﬀers several rationale for the Stein paradox. Sections 1 and 2 deﬁnes the multivariate normal mean estimation problem and introduces Stein’s paradox. Sections 3–8 advances the Galtonian perspective and explains Stein’s paradox using regression analysis. Sections 9 and 10 approaches Stein’s paradox through the conventional empirical Bayes approach. The closing sections 11 and 12 compares admissibility to equivariance and to minimaxity as criteria for simultaneous estimation.},
	language = {en},
	author = {Yung, Kwong Hiu},
}

@article{brown_geometrical_2012,
	title = {A {Geometrical} {Explanation} of {Stein} {Shrinkage}},
	volume = {27},
	issn = {0883-4237},
	url = {https://projecteuclid.org/journals/statistical-science/volume-27/issue-1/A-Geometrical-Explanation-of-Stein-Shrinkage/10.1214/11-STS382.full},
	doi = {10.1214/11-STS382},
	abstract = {Shrinkage estimation has become a basic tool in the analysis of high-dimensional data. Historically and conceptually a key development toward this was the discovery of the inadmissibility of the usual estimator of a multivariate normal mean.},
	language = {en},
	number = {1},
	urldate = {2024-01-19},
	journal = {Statistical Science},
	author = {Brown, Lawrence D. and Zhao, Linda H.},
	month = feb,
	year = {2012},
}

@article{vassend_philosophical_2017,
	title = {The philosophical significance of {Stein}’s paradox},
	volume = {7},
	issn = {1879-4912, 1879-4920},
	url = {http://link.springer.com/10.1007/s13194-016-0168-7},
	doi = {10.1007/s13194-016-0168-7},
	abstract = {Charles Stein discovered a paradox in 1955 that many statisticians think is of fundamental importance. Here we explore its philosophical implications. We outline the nature of Stein’s result and of subsequent work on shrinkage estimators; then we describe how these results are related to Bayesianism and to model selection criteria like the Akaike Information Criterion. We also discuss their bearing on scientific realism and instrumentalism. We argue that results concerning shrinkage estimators underwrite a surprising form of holistic pragmatism.},
	language = {en},
	number = {3},
	urldate = {2024-01-19},
	journal = {European Journal for Philosophy of Science},
	author = {Vassend, Olav and Sober, Elliott and Fitelson, Branden},
	month = oct,
	year = {2017},
	pages = {411--433},
}

@incollection{stein_inadmissibility_1956,
	title = {{INADMISSIBILITY} {OF} {THE} {USUAL} {ESTIMATOR} {FOR} {THE} {MEAN} {OF} {A} {MULTIVARIATE} {NORMAL} {DISTRIBUTION}},
	isbn = {978-0-520-31388-0},
	url = {https://www.degruyter.com/document/doi/10.1525/9780520313880-018/html},
	language = {en},
	urldate = {2024-01-19},
	booktitle = {Contribution to the {Theory} of {Statistics}},
	publisher = {University of California Press},
	author = {Stein, Charles},
	month = dec,
	year = {1956},
	doi = {10.1525/9780520313880-018},
	pages = {197--206},
}

@book{elmasri_fundamentals_2016,
	address = {Boston Munich},
	edition = {Seventh edition},
	title = {Fundamentals of database systems},
	isbn = {978-0-13-397077-7},
	language = {en},
	publisher = {Pearson},
	author = {Elmasri, Ramez and Navathe, Sham},
	year = {2016},
}

@article{luers_ordinal_nodate,
	title = {Ordinal {Logistic} {Regression} models and {Statistical} {Software}: {What} {You} {Need} to {Know}},
	language = {en},
	author = {Luers, Brook},
}

@article{mood_logistic_2010,
	title = {Logistic {Regression}: {Why} {We} {Cannot} {Do} {What} {We} {Think} {We} {Can} {Do}, and {What} {We} {Can} {Do} {About} {It}},
	volume = {26},
	issn = {0266-7215, 1468-2672},
	shorttitle = {Logistic {Regression}},
	url = {https://academic.oup.com/esr/article-lookup/doi/10.1093/esr/jcp006},
	doi = {10.1093/esr/jcp006},
	language = {en},
	number = {1},
	urldate = {2024-01-19},
	journal = {European Sociological Review},
	author = {Mood, C.},
	month = feb,
	year = {2010},
	pages = {67--82},
}

@misc{zheng_dags_2018-1,
	title = {{DAGs} with {NO} {TEARS}: {Continuous} {Optimization} for {Structure} {Learning}},
	shorttitle = {{DAGs} with {NO} {TEARS}},
	url = {http://arxiv.org/abs/1803.01422},
	abstract = {Estimating the structure of directed acyclic graphs (DAGs, also known as Bayesian networks) is a challenging problem since the search space of DAGs is combinatorial and scales superexponentially with the number of nodes. Existing approaches rely on various local heuristics for enforcing the acyclicity constraint. In this paper, we introduce a fundamentally different strategy: We formulate the structure learning problem as a purely {\textbackslash}emph\{continuous\} optimization problem over real matrices that avoids this combinatorial constraint entirely. This is achieved by a novel characterization of acyclicity that is not only smooth but also exact. The resulting problem can be efficiently solved by standard numerical algorithms, which also makes implementation effortless. The proposed method outperforms existing ones, without imposing any structural assumptions on the graph such as bounded treewidth or in-degree. Code implementing the proposed algorithm is open-source and publicly available at https://github.com/xunzheng/notears.},
	urldate = {2024-01-19},
	publisher = {arXiv},
	author = {Zheng, Xun and Aragam, Bryon and Ravikumar, Pradeep and Xing, Eric P.},
	month = nov,
	year = {2018},
	note = {arXiv:1803.01422 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
}

@article{stensrud_limitations_2019-1,
	title = {Limitations of hazard ratios in clinical trials},
	volume = {40},
	issn = {0195-668X, 1522-9645},
	url = {https://academic.oup.com/eurheartj/article/40/17/1378/5219649},
	doi = {10.1093/eurheartj/ehy770},
	abstract = {Abstract},
	language = {en},
	number = {17},
	urldate = {2024-01-19},
	journal = {European Heart Journal},
	author = {Stensrud, Mats J and Aalen, John M and Aalen, Odd O and Valberg, Morten},
	month = may,
	year = {2019},
	pages = {1378--1383},
}

@article{plummer_cuts_2015,
	title = {Cuts in {Bayesian} graphical models},
	volume = {25},
	issn = {0960-3174, 1573-1375},
	url = {http://link.springer.com/10.1007/s11222-014-9503-z},
	doi = {10.1007/s11222-014-9503-z},
	abstract = {The cut function deﬁned by the OpenBUGS software is described as a “valve” that prevents feedback in Bayesian graphical models. It is shown that the MCMC algorithm applied by OpenBUGS in the presence of a cut function does not converge to a well-deﬁned limiting distribution. However, it may be improved by using tempered transitions. The cut algorithm is compared with multiple imputation as a gold standard in a simple example.},
	language = {en},
	number = {1},
	urldate = {2023-11-28},
	journal = {Statistics and Computing},
	author = {Plummer, Martyn},
	month = jan,
	year = {2015},
	pages = {37--43},
}

@article{schumacher_flexible_2022,
	title = {A {FLEXIBLE} {BAYESIAN} {FRAMEWORK} {TO} {ESTIMATE} {AGE}- {AND} {CAUSE}-{SPECIFIC} {CHILD} {MORTALITY} {OVER} {TIME} {FROM} {SAMPLE} {REGISTRATION} {DATA}},
	volume = {16},
	issn = {1932-6157},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10448806/},
	doi = {10.1214/21-aoas1489},
	abstract = {In order to implement disease-specific interventions in young age groups, policy makers in low- and middle-income countries require timely and accurate estimates of age- and cause-specific child mortality. High-quality data is not available in settings where these interventions are most needed, but there is a push to create sample registration systems that collect detailed mortality information. current methods that estimate mortality from this data employ multistage frameworks without rigorous statistical justification that separately estimate all-cause and cause-specific mortality and are not sufficiently adaptable to capture important features of the data. We propose a flexible Bayesian modeling framework to estimate age- and cause-specific child mortality from sample registration data. We provide a theoretical justification for the framework, explore its properties via simulation, and use it to estimate mortality trends using data from the Maternal and Child Health Surveillance System in China.},
	number = {1},
	urldate = {2023-11-28},
	journal = {The annals of applied statistics},
	author = {Schumacher, Austin E. and McCormick, Tyler H. and Wakefield, Jon and Chu, Yue and Perin, Jamie and Villavicencio, Francisco and Simon, Noah and Liu, Li},
	month = mar,
	year = {2022},
	pmid = {37621750},
	pmcid = {PMC10448806},
	pages = {124--143},
}

@article{zafar_measuring_2022,
	title = {Measuring diachronic sense change: new models and {Monte} {Carlo} methods for {Bayesian} inference},
	volume = {71},
	issn = {0035-9254, 1467-9876},
	shorttitle = {Measuring diachronic sense change},
	url = {http://arxiv.org/abs/2105.00819},
	doi = {10.1111/rssc.12591},
	abstract = {In a bag-of-words model, the senses of a word with multiple meanings, e.g. "bank" (used either in a river-bank or an institution sense), are represented as probability distributions over context words, and sense prevalence is represented as a probability distribution over senses. Both of these may change with time. Modelling and measuring this kind of sense change is challenging due to the typically high-dimensional parameter space and sparse datasets. A recently published corpus of ancient Greek texts contains expert-annotated sense labels for selected target words. Automatic sense-annotation for the word "kosmos" (meaning decoration, order or world) has been used as a test case in recent work with related generative models and Monte Carlo methods. We adapt an existing generative sense change model to develop a simpler model for the main effects of sense and time, and give MCMC methods for Bayesian inference on all these models that are more efficient than existing methods. We carry out automatic sense-annotation of snippets containing "kosmos" using our model, and measure the time-evolution of its three senses and their prevalence. As far as we are aware, ours is the first analysis of this data, within the class of generative models we consider, that quantifies uncertainty and returns credible sets for evolving sense prevalence in good agreement with those given by expert annotation.},
	number = {5},
	urldate = {2023-11-28},
	journal = {Journal of the Royal Statistical Society Series C: Applied Statistics},
	author = {Zafar, Schyan and Nicholls, Geoff},
	month = nov,
	year = {2022},
	note = {arXiv:2105.00819 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Statistics - Methodology},
	pages = {1569--1604},
}

@article{alsing_fast_2019,
	title = {Fast likelihood-free cosmology with neural density estimators and active learning},
	issn = {0035-8711, 1365-2966},
	url = {http://arxiv.org/abs/1903.00007},
	doi = {10.1093/mnras/stz1960},
	abstract = {Likelihood-free inference provides a framework for performing rigorous Bayesian inference using only forward simulations, properly accounting for all physical and observational effects that can be successfully included in the simulations. The key challenge for likelihood-free applications in cosmology, where simulation is typically expensive, is developing methods that can achieve high-fidelity posterior inference with as few simulations as possible. Density-estimation likelihood-free inference (DELFI) methods turn inference into a density estimation task on a set of simulated data-parameter pairs, and give orders of magnitude improvements over traditional Approximate Bayesian Computation approaches to likelihood-free inference. In this paper we use neural density estimators (NDEs) to learn the likelihood function from a set of simulated datasets, with active learning to adaptively acquire simulations in the most relevant regions of parameter space on-the-fly. We demonstrate the approach on a number of cosmological case studies, showing that for typical problems high-fidelity posterior inference can be achieved with just \${\textbackslash}mathcal\{O\}(10{\textasciicircum}3)\$ simulations or fewer. In addition to enabling efficient simulation-based inference, for simple problems where the form of the likelihood is known, DELFI offers a fast alternative to MCMC sampling, giving orders of magnitude speed-up in some cases. Finally, we introduce {\textbackslash}textsc\{pydelfi\} -- a flexible public implementation of DELFI with NDEs and active learning -- available at {\textbackslash}url\{https://github.com/justinalsing/pydelfi\}.},
	urldate = {2023-11-28},
	journal = {Monthly Notices of the Royal Astronomical Society},
	author = {Alsing, Justin and Charnock, Tom and Feeney, Stephen and Wandelt, Benjamin},
	month = jul,
	year = {2019},
	note = {arXiv:1903.00007 [astro-ph]},
	keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics},
	pages = {stz1960},
}

@misc{murray_mcmc_2012,
	title = {{MCMC} for doubly-intractable distributions},
	url = {http://arxiv.org/abs/1206.6848},
	doi = {10.48550/arXiv.1206.6848},
	abstract = {Markov Chain Monte Carlo (MCMC) algorithms are routinely used to draw samples from distributions with intractable normalization constants. However, standard MCMC algorithms do not apply to doubly-intractable distributions in which there are additional parameter-dependent normalization terms; for example, the posterior over parameters of an undirected graphical model. An ingenious auxiliary-variable scheme (Moeller et al., 2004) offers a solution: exact sampling (Propp and Wilson, 1996) is used to sample from a Metropolis-Hastings proposal for which the acceptance probability is tractable. Unfortunately the acceptance probability of these expensive updates can be low. This paper provides a generalization of Moeller et al. (2004) and a new MCMC algorithm, which obtains better acceptance probabilities for the same amount of exact sampling, and removes the need to estimate model parameters before sampling begins.},
	urldate = {2023-11-28},
	publisher = {arXiv},
	author = {Murray, Iain and Ghahramani, Zoubin and MacKay, David},
	month = jun,
	year = {2012},
	note = {arXiv:1206.6848 [stat]},
	keywords = {Statistics - Computation, Statistics - Methodology},
}

@misc{bartlett_advances_2015,
	title = {Advances in {Bayesian} {Network} {Learning} using {Integer} {Programming}},
	url = {http://arxiv.org/abs/1309.6825},
	doi = {10.48550/arXiv.1309.6825},
	abstract = {We consider the problem of learning Bayesian networks (BNs) from complete discrete data. This problem of discrete optimisation is formulated as an integer program (IP). We describe the various steps we have taken to allow efficient solving of this IP. These are (i) efficient search for cutting planes, (ii) a fast greedy algorithm to find high-scoring (perhaps not optimal) BNs and (iii) tightening the linear relaxation of the IP. After relating this BN learning problem to set covering and the multidimensional 0-1 knapsack problem, we present our empirical results. These show improvements, sometimes dramatic, over earlier results.},
	urldate = {2023-11-28},
	publisher = {arXiv},
	author = {Bartlett, Mark and Cussens, James},
	month = mar,
	year = {2015},
	note = {arXiv:1309.6825 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@article{rodraguez_nested_2008,
	title = {The {Nested} {Dirichlet} {Process}},
	volume = {103},
	doi = {10.1198/016214508000000553},
	abstract = {In multicenter studies, subjects in different centers may have different outcome distri- butions. This article is motivated by the problem of nonparametric modeling of these distributions, borrowing information across centers while also allowing centers to be clustered. Starting with a stick- breaking representation of the Dirichlet process (DP), we replace the random atoms with random prob- ability measures drawn from a DP. This results in a nested Dirichlet process (nDP) prior, which can be placed on the collection of distributions for the different centers, with centers drawn from the same DP component automatically clustered together. Theoretical properties are discussed, and an efficient MCMC algorithm is developed for computation. The methods are illustrated using a simulation study and an application to quality of care in US hospitals.},
	journal = {Journal of the American Statistical Association},
	author = {RodrÃ­guez, Abel and Dunson, David and Gelfand, Alan},
	month = sep,
	year = {2008},
	pages = {1131--1154},
}

@article{filipe_bayesian_2019,
	title = {Bayesian, {Likelihood}-{Free} {Modelling} of {Phenotypic} {Plasticity} and {Variability} in {Individuals} and {Populations}},
	volume = {10},
	issn = {1664-8021},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6764410/},
	doi = {10.3389/fgene.2019.00727},
	abstract = {There is a paradigm shift from the traditional focus on the “average” individual towards the definition and analysis of trait variation within individual life-history and among individuals in populations. This is a result of increasing availability of individual phenotypic data. The shift allows the use of genetic and environment-driven variations to assess robustness to challenge, gain greater understanding of organismal biological processes, or deliver individual-targeted treatments or genetic selection. These consequences apply, in particular, to variation in ontogenetic growth. We propose an approach to parameterise mathematical models of individual traits (e.g., reaction norms, growth curves) that address two challenges: 1) Estimation of individual traits while making minimal assumptions about data distribution and correlation, addressed via Approximate Bayesian Computation (a form of nonparametric inference). We are motivated by the fact that available information on distribution of biological data is often less precise than assumed by conventional likelihood functions. 2) Scaling-up to population phenotype distributions while facilitating unbiased use of individual data; this is addressed via a probabilistic framework where population distributions build on separately-inferred individual distributions and individual-trait interpretability is preserved. The approach is tested against Bayesian likelihood-based inference, by fitting weight and energy intake growth models to animal data and normal- and skewed-distributed simulated data. i) Individual inferences were accurate and robust to changes in data distribution and sample size; in particular, median-based predictions were more robust than maximum- likelihood-based curves. These results suggest that the approach gives reliable inferences using few observations and monitoring resources. ii) At the population level, each individual contributed via a specific data distribution, and population phenotype estimates were not disproportionally influenced by outlier individuals. Indices measuring population phenotype variation can be derived for study comparisons. The approach offers an alternative for estimating trait variability in biological systems that may be reliable for various applications, for example, in genetics, health, and individualised nutrition, while using fewer assumptions and fewer empirical observations. In livestock breeding, the potentially greater accuracy of trait estimation (without specification of multitrait variance-covariance parameters) could lead to improved selection and to more decisive estimates of trait heritability.},
	urldate = {2023-11-28},
	journal = {Frontiers in Genetics},
	author = {Filipe, Joao A.N. and Kyriazakis, Ilias},
	month = sep,
	year = {2019},
	pmid = {31616460},
	pmcid = {PMC6764410},
	pages = {727},
}

@article{jiang_learning_2018,
	title = {Learning {Summary} {Statistic} for {Approximate} {Bayesian} {Computation} via {Deep} {Neural} {Network}},
	issn = {10170405},
	url = {http://arxiv.org/abs/1510.02175},
	doi = {10.5705/ss.202015.0340},
	abstract = {Approximate Bayesian Computation (ABC) methods are used to approximate posterior distributions in models with unknown or computationally intractable likelihoods. Both the accuracy and computational efficiency of ABC depend on the choice of summary statistic, but outside of special cases where the optimal summary statistics are known, it is unclear which guiding principles can be used to construct effective summary statistics. In this paper we explore the possibility of automating the process of constructing summary statistics by training deep neural networks to predict the parameters from artificially generated data: the resulting summary statistics are approximately posterior means of the parameters. With minimal model-specific tuning, our method constructs summary statistics for the Ising model and the moving-average model, which match or exceed theoretically-motivated summary statistics in terms of the accuracies of the resulting posteriors.},
	urldate = {2023-11-28},
	journal = {Statistica Sinica},
	author = {Jiang, Bai and Wu, Tung-yu and Zheng, Charles and Wong, Wing H.},
	year = {2018},
	note = {arXiv:1510.02175 [stat]},
	keywords = {Statistics - Computation, Statistics - Machine Learning, Statistics - Methodology},
}

@misc{dax_flow_2023,
	title = {Flow {Matching} for {Scalable} {Simulation}-{Based} {Inference}},
	url = {http://arxiv.org/abs/2305.17161},
	doi = {10.48550/arXiv.2305.17161},
	abstract = {Neural posterior estimation methods based on discrete normalizing flows have become established tools for simulation-based inference (SBI), but scaling them to high-dimensional problems can be challenging. Building on recent advances in generative modeling, we here present flow matching posterior estimation (FMPE), a technique for SBI using continuous normalizing flows. Like diffusion models, and in contrast to discrete flows, flow matching allows for unconstrained architectures, providing enhanced flexibility for complex data modalities. Flow matching, therefore, enables exact density evaluation, fast training, and seamless scalability to large architectures--making it ideal for SBI. We show that FMPE achieves competitive performance on an established SBI benchmark, and then demonstrate its improved scalability on a challenging scientific problem: for gravitational-wave inference, FMPE outperforms methods based on comparable discrete flows, reducing training time by 30\% with substantially improved accuracy. Our work underscores the potential of FMPE to enhance performance in challenging inference scenarios, thereby paving the way for more advanced applications to scientific problems.},
	urldate = {2023-11-28},
	publisher = {arXiv},
	author = {Dax, Maximilian and Wildberger, Jonas and Buchholz, Simon and Green, Stephen R. and Macke, Jakob H. and Schölkopf, Bernhard},
	month = oct,
	year = {2023},
	note = {arXiv:2305.17161 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{romano_conformalized_2019,
	title = {Conformalized {Quantile} {Regression}},
	volume = {32},
	url = {https://papers.nips.cc/paper_files/paper/2019/hash/5103c3584b063c431bd1268e9b5e76fb-Abstract.html},
	abstract = {Conformal prediction is a technique for constructing prediction intervals that attain valid coverage in finite samples, without making distributional assumptions. Despite this appeal, existing conformal methods can be unnecessarily conservative because they form intervals of constant or weakly varying length across the input space. In this paper we propose a new method that is fully adaptive to heteroscedasticity. It combines conformal prediction with classical quantile regression, inheriting the advantages of both. We establish a theoretical guarantee of valid coverage, supplemented by extensive experiments on popular regression datasets. We compare the efficiency of conformalized quantile regression to other conformal methods, showing that our method tends to produce shorter intervals.},
	urldate = {2023-11-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Romano, Yaniv and Patterson, Evan and Candes, Emmanuel},
	year = {2019},
}

@misc{lin_conformal_2022,
	title = {Conformal {Prediction} with {Temporal} {Quantile} {Adjustments}},
	url = {http://arxiv.org/abs/2205.09940},
	abstract = {We develop Temporal Quantile Adjustment (TQA), a general method to construct efficient and valid prediction intervals (PIs) for regression on cross-sectional time series data. Such data is common in many domains, including econometrics and healthcare. A canonical example in healthcare is predicting patient outcomes using physiological time-series data, where a population of patients composes a cross-section. Reliable PI estimators in this setting must address two distinct notions of coverage: cross-sectional coverage across a cross-sectional slice, and longitudinal coverage along the temporal dimension for each time series. Recent works have explored adapting Conformal Prediction (CP) to obtain PIs in the time series context. However, none handles both notions of coverage simultaneously. CP methods typically query a pre-specified quantile from the distribution of nonconformity scores on a calibration set. TQA adjusts the quantile to query in CP at each time \$t\$, accounting for both cross-sectional and longitudinal coverage in a theoretically-grounded manner. The post-hoc nature of TQA facilitates its use as a general wrapper around any time series regression model. We validate TQA's performance through extensive experimentation: TQA generally obtains efficient PIs and improves longitudinal coverage while preserving cross-sectional coverage.},
	urldate = {2023-11-28},
	publisher = {arXiv},
	author = {Lin, Zhen and Trivedi, Shubhendu and Sun, Jimeng},
	month = may,
	year = {2022},
	note = {arXiv:2205.09940 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
}

@misc{jewson_stability_2023,
	title = {On the {Stability} of {General} {Bayesian} {Inference}},
	url = {http://arxiv.org/abs/2301.13701},
	doi = {10.48550/arXiv.2301.13701},
	abstract = {We study the stability of posterior predictive inferences to the specification of the likelihood model and perturbations of the data generating process. In modern big data analyses, the decision-maker may elicit useful broad structural judgements but a level of interpolation is required to arrive at a likelihood model. One model, often a computationally convenient canonical form, is chosen, when many alternatives would have been equally consistent with the elicited judgements. Equally, observational datasets often contain unforeseen heterogeneities and recording errors. Acknowledging such imprecisions, a faithful Bayesian analysis should be stable across reasonable equivalence classes for these inputs. We show that traditional Bayesian updating provides stability across a very strict class of likelihood models and DGPs, while a generalised Bayesian alternative using the beta-divergence loss function is shown to be stable across practical and interpretable neighbourhoods. We illustrate this in linear regression, binary classification, and mixture modelling examples, showing that stable updating does not compromise the ability to learn about the DGP. These stability results provide a compelling justification for using generalised Bayes to facilitate inference under simplified canonical models.},
	urldate = {2023-11-28},
	publisher = {arXiv},
	author = {Jewson, Jack and Smith, Jim Q. and Holmes, Chris},
	month = jan,
	year = {2023},
	note = {arXiv:2301.13701 [stat]},
	keywords = {Statistics - Methodology},
}

@article{anderson_kernel_2009,
	title = {Kernel density estimation and {K}-means clustering to profile road accident hotspots},
	volume = {41},
	issn = {00014575},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0001457508002340},
	doi = {10.1016/j.aap.2008.12.014},
	abstract = {Identifying road accident hotspots is a key role in determining effective strategies for the reduction of high density areas of accidents. This paper presents (1) a methodology using Geographical Information Systems (GIS) and Kernel Density Estimation to study the spatial patterns of injury related road accidents in London, UK and (2) a clustering methodology using environmental data and results from the ﬁrst section in order to create a classiﬁcation of road accident hotspots. The use of this methodology will be illustrated using the London area in the UK. Road accident data collected by the Metropolitan Police from 1999 to 2003 was used. A kernel density estimation map was created and subsequently disaggregated by cell density to create a basic spatial unit of an accident hotspot. Appended environmental data was then added to the hotspot cells and using K-means clustering, an outcome of similar hotspots was deciphered. Five groups and 15 clusters were created based on collision and attribute data. These clusters are discussed and evaluated according to their robustness and potential uses in road safety campaigning.},
	language = {en},
	number = {3},
	urldate = {2023-11-23},
	journal = {Accident Analysis \& Prevention},
	author = {Anderson, Tessa K.},
	month = may,
	year = {2009},
	pages = {359--364},
}

@article{duguay_assessing_2023,
	title = {Assessing risk factors for malaria and schistosomiasis among children in {Misungwi}, {Tanzania}, an area of co-endemicity: {A} mixed methods study},
	volume = {3},
	issn = {2767-3375},
	shorttitle = {Assessing risk factors for malaria and schistosomiasis among children in {Misungwi}, {Tanzania}, an area of co-endemicity},
	url = {https://journals.plos.org/globalpublichealth/article?id=10.1371/journal.pgph.0002468},
	doi = {10.1371/journal.pgph.0002468},
	abstract = {Malaria and schistosomiasis are two major parasitic vector-borne diseases that are a particular threat to young children in Sub-Saharan Africa. In the present study, we investigated factors that are associated with malaria, schistosomiasis, and co-infection among school-aged children, using an explanatory sequential mixed-methods approach. A cross-sectional study was conducted in January 2022 in Misungwi, Tanzania, that sampled 1,122 children aged 5 to 14 years old for malaria and schistosomiasis infection. Mixed-effect logistic regression models were used to assess the association between infection prevalence or seroprevalence, and environmental determinants that create favorable conditions for vectors and parasites and social determinants that relate to disease exposure. Community mapping combined with direct field observations were conducted in August 2022 in three selected villages from the cross-sectional study to understand specific water use behaviors and to identify potential malaria mosquito larval breeding sites and freshwater snail habitat. The prevalence of malaria, seroprevalence of schistosomiasis, and co-infection in this study were 40.4\%, 94.3\%, and 38.1\%, respectively. Individual-level factors emerged as the primary determinants driving the association with infection, with age (every one-year increase in age) and sex (boys vs girls) being statistically and positively associated with malaria, schistosomiasis, and co-infection (P{\textless}0.05 for all). Community maps identified many unimproved water sources in all three villages that were used by humans, cattle, or both. We found that children primarily fetched water, and that unprotected wells were dedicated for drinking water whereas ponds were dedicated for other domestic uses and cattle. Although not identified in the community maps, we found hand pumps in all three villages were not in use because of unpleasant taste and high cost. This study improves our understanding of individual, social and environmental factors that are associated with malaria, schistosomiasis, and co-infection, which can inform potential entry points for integrated disease prevention and control.},
	language = {en},
	number = {11},
	urldate = {2023-11-23},
	journal = {PLOS Global Public Health},
	author = {Duguay, Claudia and Mosha, Jacklin F. and Lukole, Eliud and Mangalu, Doris and Thickstun, Charles and Mallya, Elizabeth and Aziz, Tatu and Feng, Cindy and Protopopoff, Natacha and Mosha, Franklin and Manjurano, Alphaxard and Krentel, Alison and Kulkarni, Manisha A.},
	month = nov,
	year = {2023},
	note = {Publisher: Public Library of Science},
	keywords = {Co-infections, Enzyme-linked immunoassays, Malaria, Medical risk factors, Ponds, Sanitation, Schistosomiasis, Tanzania},
	pages = {e0002468},
}

@misc{noauthor_world_nodate,
	title = {World malaria report 2022},
	url = {https://www.who.int/publications-detail-redirect/9789240064898},
	abstract = {The report highlights progress towards global targets and describes opportunities and challenges for curbing and eliminating malaria},
	language = {en},
	urldate = {2023-11-23},
}

@misc{noauthor_world_nodate-1,
	title = {World malaria report 2022},
	url = {https://www.who.int/publications-detail-redirect/9789240064898},
	abstract = {The report highlights progress towards global targets and describes opportunities and challenges for curbing and eliminating malaria},
	language = {en},
	urldate = {2023-11-23},
}

@article{ohara_review_2009,
	title = {A review of {Bayesian} variable selection methods: what, how and which},
	volume = {4},
	issn = {1936-0975, 1931-6690},
	shorttitle = {A review of {Bayesian} variable selection methods},
	url = {https://projecteuclid.org/journals/bayesian-analysis/volume-4/issue-1/A-review-of-Bayesian-variable-selection-methods--what-how/10.1214/09-BA403.full},
	doi = {10.1214/09-BA403},
	abstract = {The selection of variables in regression problems has occupied the minds of many statisticians. Several Bayesian variable selection methods have been developed, and we concentrate on the following methods: Kuo \& Mallick, Gibbs Variable Selection (GVS), Stochastic Search Variable Selection (SSVS), adaptive shrinkage with Jeffreys' prior or a Laplacian prior, and reversible jump MCMC. We review these methods, in the context of their different properties. We then implement the methods in BUGS, using both real and simulated data as examples, and investigate how the different methods perform in practice. Our results suggest that SSVS, reversible jump MCMC and adaptive shrinkage methods can all work well, but the choice of which method is better will depend on the priors that are used, and also on how they are implemented.},
	number = {1},
	urldate = {2023-11-22},
	journal = {Bayesian Analysis},
	author = {O'Hara, R. B. and Sillanpää, M. J.},
	month = mar,
	year = {2009},
	note = {Publisher: International Society for Bayesian Analysis},
	keywords = {BUGS, MCMC, Variable selection},
	pages = {85--117},
}

@article{forte_methods_nodate,
	title = {Methods and {Tools} for {Bayesian} {Variable} {Selection} and {Model} {Averaging} in {Univariate} {Linear} {Regression}},
	abstract = {In this paper we brieﬂy review the main methodological aspects concerned with the application of the Bayesian approach to model choice and model averaging in the context of variable selection in regression models. This includes prior elicitation, summaries of the posterior distribution and computational strategies. We then examine and compare various publicly available R-packages for its practical implementation summarizing and explaining the diﬀerences between packages and giving recommendations for applied users. We ﬁnd that all packages reviewed lead to very similar results, but there are potentially important diﬀerences in ﬂexibility and eﬃciency of the packages.},
	language = {en},
	author = {Forte, Anabel and Garcıa-Donato, Gonzalo},
}

@misc{anjorin_epidemiology_2023,
	title = {The epidemiology of periportal fibrosis and relevance of current {Schistosoma} mansoni infection: a population-based, cross-sectional study},
	copyright = {© 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	shorttitle = {The epidemiology of periportal fibrosis and relevance of current {Schistosoma} mansoni infection},
	url = {https://www.medrxiv.org/content/10.1101/2023.09.15.23295612v1},
	doi = {10.1101/2023.09.15.23295612},
	abstract = {Background Intestinal schistosome infections are known to cause periportal fibrosis (PPF). Yet, the epidemiology of PPF remains poorly understood, especially in settings endemic with Schistosoma mansoni.
Methods We randomly sampled 1442 households from 38 villages in Mayuge, Buliisa, and Pakwach Districts of Uganda within the SchistoTrack Cohort to examine 2834 individuals aged 5-90 years. PPF was diagnosed using ultrasound and image patterns C-F from the Niamey Protocol. S. mansoni infection status/intensity was diagnosed by Kato-Katz microscopy and point-of-care circulating cathodic antigens (POC-CCA). Schistosome infection, coinfections, and comorbidities were examined as exposures for PPF. Logistic regressions were run with standard errors clustered by household.
Findings PPF prevalence was 12·10\% (343/2834), varying from 5·00-19·46\% across districts. S. mansoni prevalence by Kato-Katz, and POC-CCA trace negative and positive was 43·37\% (1229/2834), 40·86\% (1158/2834), and 65·73\% (1863/2834) respectively. Individual schistosome infection status/intensity was not correlated with the likelihood of PPF. Living in a village where adults had {\textless}5\% prevalence of heavy intensity infections (400+ eggs per gram of stool) was associated with 30.2\% decreased odds of PPF. The likelihood of PPF with age linearly increased from 5-25, exponentially changed from 26-45, remain unchanged from 45- 60, and steadily decreased past 60 years. History of liver diseases, human immunodeficiency virus positivity (HIV+), and ultrasound-detected chronic hepatitis/early cirrhosis-like disease were associated with {\textgreater}2-fold increased PPF likelihood.
Interpretation Current individual schistosome infections alone are uninformative for PPF. History of HIV+ and underlying chronic hepatitis/early cirrhosis-like disease were risk factors and could be investigated for PPF surveillance and management.
Funding Nuffield Department of Population Health Pump Priming Fund, Wellcome Trust Institutional Strategic Support Fund (204826/Z/16/Z), John Fell Fund, Robertson Foundation Fellowship, and UKRI EPSRC Award (EP/X021793/1).
Evidence before this study Morbidity due to parasitic infection is a complex interplay of current and past exposures. World Health Organization (WHO) guidelines for elimination of schistosomiasis as a public health problem assume current infection is a reliable proxy indicator of prevalent morbidity. Community infection thresholds are defined in guidelines and, when met, the assumption is there is no schistosomiasis-related morbidity. There is a lack of evidence for the association of infection with prevalent morbidity in the context of repeated treatment from routine mass drug administration. To evaluate WHO guidelines, there is a need for large-scale population- based, cross-sectional studies in endemic areas where current infection is compared with current morbidity at the same timepoint. A cross-sectional design also enables the investigation of a wide range of risk factors to assess the relative importance of current schistosome infection. Periportal fibrosis is a schistosomiasis-associated severe morbidity with clinical consequences such as portal hypertension, upper gastrointestinal tract bleeding, and ultimately premature death. Yet, little is known about the distribution of this disease; no information is available on its most basic epidemiology including age and gender-specific likelihoods. It is schistosomiasis-specific or attributable to schistosome pathology unlike more subtle conditions with complex aetiologies (e.g. anaemia). Hence, investigating periportal fibrosis serves as a first line, conservative approach to evaluating World Health Organization guidelines for elimination of schistosomiasis-related morbidity as a public health problem. A systematic literature search as part of an ongoing metanalysis was prospectively registered on PROSPERO (CRD42022333919). Databases were searched on 18th May 2022 and included the Cochrane Central Register of Controlled Trials, Embase, Global Health, Global Index Medicus, and Medline. The following general terms were used: “Schistosoma” AND “fibrosis” AND “intensity” AND “infection” AND “periportal OR liver”. Studies were included that considered Schistosoma mansoni, S. japonicum, or S. mekongi species. Only original research articles in English were considered. No restriction on date of publication, age, gender, or region was applied. Infection was required to be diagnosed as opposed to self-reported. Periportal fibrosis was defined by the study authors.Added value of this study No population-wide or adjusted analyses were found to characterize the age-specific likelihood of periportal fibrosis. Studies focused on unadjusted associations in nonrandomly sampled populations of narrow age groups. In a single case (Weigand et al 2021) where adjusted analyses were completed (with age and gender considered), national programmatic data was used with sparsely nonrandomly sampled schoolchildren and limited frequency of the outcome of periportal fibrosis. Ultrasound data collection protocols and validation across studies were poorly reported. There was a lack of investigations on coinfections and comorbidities with only eight studies initiated (or published) from 2003 onwards after the start of mass drug administration in sub-Saharan Africa.Implications of all available evidence To our knowledge, this study is the first to characterize the epidemiology of periportal fibrosis with respect to the most common intestinal schistosome pathogen (S. mansoni). We conducted a comprehensive, population-based study of all ages (5+ years) eligible for mass drug administration in an area that has received at least 13 rounds of treatment. Here we provide clear evidence for the lack of association of current S. mansoni infection status and intensity with periportal fibrosis irrespective of the diagnostic for schistosome infection. No support was found for current WHO elimination guidelines despite using arguably the most biologically specific and severe morbidity associated with schistosomiasis. We also characterized the age-specific likelihood of periportal fibrosis, identifying a transitional age as young as 25 years. We identified future avenues for research into coinfections such as HIV and hepatitis B that appear to influence periportal fibrosis status even after controlling for a wide range of biosocial determinants of schistosome infection, treatment, and unrelated liver fibrosis. Future work is needed to understand if/how coinfections alter the pathogenesis of periportal fibrosis. Importantly, World Health Organization guidelines should be differentiated for schistosomiasis morbidities to discourage the use of infection status/intensity/prevalence as a proxy indicator for monitoring the elimination of periportal fibrosis as a public health problem.},
	language = {en},
	urldate = {2023-11-22},
	publisher = {medRxiv},
	author = {Anjorin, Seun and Nabatte, Betty and Mpooya, Simon and Tinkitina, Benjamin and Opio, Christopher K. and Kabatereine, Narcis B. and Chami, Goylette F.},
	month = sep,
	year = {2023},
	note = {Pages: 2023.09.15.23295612},
}

@article{bisanzio_cross-sectional_2014,
	title = {Cross-sectional {Study} of the {Burden} of {Vector}-{Borne} and {Soil}-{Transmitted} {Polyparasitism} in {Rural} {Communities} of {Coast} {Province}, {Kenya}},
	volume = {8},
	issn = {1935-2735},
	url = {https://journals.plos.org/plosntds/article?id=10.1371/journal.pntd.0002992},
	doi = {10.1371/journal.pntd.0002992},
	abstract = {Background In coastal Kenya, infection of human populations by a variety of parasites often results in co-infection or poly-parasitism. These parasitic infections, separately and in conjunction, are a major cause of chronic clinical and sub-clinical human disease and exert a long-term toll on economic welfare of affected populations. Risk factors for these infections are often shared and overlap in space, resulting in interrelated patterns of transmission that need to be considered at different spatial scales. Integration of novel quantitative tools and qualitative approaches is needed to analyze transmission dynamics and design effective interventions. Methodology Our study was focused on detecting spatial and demographic patterns of single- and co-infection in six villages in coastal Kenya. Individual and household level data were acquired using cross-sectional, socio-economic, and entomological surveys. Generalized additive models (GAMs and GAMMs) were applied to determine risk factors for infection and co-infections. Spatial analysis techniques were used to detect local clusters of single and multiple infections. Principal findings Of the 5,713 tested individuals, more than 50\% were infected with at least one parasite and nearly 20\% showed co-infections. Infections with Schistosoma haematobium (26.0\%) and hookworm (21.4\%) were most common, as was co-infection by both (6.3\%). Single and co-infections shared similar environmental and socio-demographic risk factors. The prevalence of single and multiple infections was heterogeneous among and within communities. Clusters of single and co-infections were detected in each village, often spatially overlapped, and were associated with lower SES and household crowding. Conclusion Parasitic infections and co-infections are widespread in coastal Kenya, and their distributions are heterogeneous across landscapes, but inter-related. We highlighted how shared risk factors are associated with high prevalence of single infections and can result in spatial clustering of co-infections. Spatial heterogeneity and synergistic risk factors for polyparasitism need to be considered when designing surveillance and intervention strategies.},
	language = {en},
	number = {7},
	urldate = {2023-11-22},
	journal = {PLOS Neglected Tropical Diseases},
	author = {Bisanzio, Donal and Mutuku, Francis and Bustinduy, Amaya L. and Mungai, Peter L. and Muchiri, Eric M. and King, Charles H. and Kitron, Uriel},
	month = jul,
	year = {2014},
	note = {Publisher: Public Library of Science},
	keywords = {Co-infections, Helminth infections, Hookworms, Malaria, Malarial parasites, Medical risk factors, Mosquitoes, Parasitic diseases},
	pages = {e2992},
}
