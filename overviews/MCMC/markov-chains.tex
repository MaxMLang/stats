\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[version=4]{mhchem}
\usepackage{stmaryrd}
\usepackage{bbold}

\title{Elements of Markov Chains Theory}

\begin{document}
\maketitle
\section{An introduction to Markov chain Monte Carlo Methods}
\subsection{Markov Chains}
Let $\left\{X_t\right\}_{t=0}^{\infty}$ be a homogeneous Markov chain of random variables on $\Omega$ with starting distribution $X_0 \sim p^{(0)}$ and transition probability matrix $P=\left(P_{i, j}\right)_{i, j \in \Omega}$ with
$$
P_{i, j}=\mathbb{P}\left(X_{t+1}=j \mid X_t=i\right) .
$$

Denote by $P_{i, j}^{(n)}$ the $n$-step transition probabilities
$$
P_{i, j}^{(n)}=\mathbb{P}\left(X_{t+n}=j \mid X_t=i\right)
$$
and by $p_i^{(n)}=\mathbb{P}\left(X_n=i\right)$, with $p^{(n)}$ a row vector.
The transition matrix $P$ is irreducible if and only if, for each pair of states $i, j \in \Omega$ there is $n$ such that $P_{i, j}^{(n)}>0$. 
\newline
\textbf{Intuition} \newline
Imagine a city with many different places you could visit — these places are like the "states" in a state space \( \Omega \). Now, the roads between these places are like the probabilities in the transition matrix \( P \). When you're at one place (state \( i \)), you can take various roads (transitions) to get to other places (states). Some roads might take you directly to your destination, while others might require you to pass through multiple locations before reaching your end point.

The city is considered "connected" (or in our case, the transition matrix \( P \) is "irreducible") if you can eventually get from any place to any other place, possibly via a series of roads. It doesn't matter how many intermediate stops (transitions) you need to make; what matters is that it's possible to reach any destination from any starting point. 

In mathematical terms, for each pair of states \( i \) and \( j \), there exists some number of steps \( n \) such that you can get from state \( i \) to state \( j \) with a positive probability after exactly \( n \) steps. This is what \( P_{i, j}^{(n)}>0 \) represents — there's a road (sequence of transitions) you can follow to go from place \( i \) to place \( j \) in \( n \) steps with a probability greater than zero. If this is true for every pair of places in the city, then there are no isolated parts of the city, and you have the freedom to eventually reach any location from any starting point.


The Markov chain is aperiodic if $P_{i, j}^{(n)}$ is non zero for all sufficiently large $n$.
\newline
\textbf{Intuition} \newline
Imagine that you have a routine for your walk. If your routine is very strict, such as going around the block every 3 days and only every 3 days, then your walking schedule is periodic with a period of 3 days. You can predict exactly which days you'll be walking. In the context of a Markov chain, this would be like saying that you can only transition from state \( i \) to state \( j \) every \( k \) steps (where \( k \) is the period).

Now, let's say you shake things up a bit. You still go for walks regularly, but now there's no strict pattern. Some weeks you might walk around the block after 2 days, sometimes after 5 days, and other times even consecutive days. If someone were to observe your walking schedule, they wouldn't be able to discern a repeating pattern that dictates when you'll walk. In essence, you have an \textit{aperiodic} walking schedule.

Translating this back to Markov chains, if the chain is \textit{aperiodic}, it means that once you're in state \( i \), the probability of being in state \( j \) isn't restricted to a fixed number of steps. For all sufficiently large numbers of steps \( n \), there's a chance (non-zero probability) you could transition from state \( i \) to state \( j \). There's no fixed cycle dictating when these transitions must happen, just like your \textit{aperiodic} walking schedule.

\subsubsection{Stationary Distribution and detailed Balance}
When we come to apply the MCMC methods to Bayesian inference, the target distribution will be the posterior $p(\theta)=\pi(\theta \mid y)$.
The probability mass function (PMF) $p_i, i \in \Omega, \sum_{i \in \Omega} p_i=1$ is a \textbf{stationary} distribution of $P$ if $p P=p$ in the following sense. If $p^{(0)}=p$ then by the partition theorem for probability (PTP),
$$
p_j^{(1)}=\sum_{i \in \Omega} p_i^{(0)} P_{i, j},
$$
so $p_j^{(1)}=p_j$ also. Iterating, $p^{(t)}=p$ for each $t=1,2, \ldots$ in the chain, so the distribution of \textbf{$X_t \sim p^{(t)}$ doesn't change with $t$, it is stationary.}
\newline
\textbf{Detailed balance (discrete case)}: If there is a probability mass function $p_i, i \in \Omega$ satisfying $\sum_{i \in \Omega} p_i=1$ and
$$
p_i P_{i, j}=p_j P_{j, i} \quad \text { holds for all } i, j \in \Omega,
$$
then $P$ and $p$ satisfy detailed balance.
\newline
\newline
\textbf{Detailed balance is sufficient for stationarity}, and it is much easier to check than $p P=p$ as it is a simple algebraic relation. A Markov chain satisfying DB is \textbf{reversible}.


\subsubsection{Convergence and Ergodic Theorem}
\textbf{Intuitive Explanation}
\newline
To intuitively understand the Ergodic Theorem for Markov chains, let's compare it to a very diligent birdwatcher.

Imagine a birdwatcher who wants to understand the proportion of time different bird species spend in various areas of a large park. Each area of the park represents a state in the Markov chain, and each bird species can move from one area to another, representing transitions between states. The birdwatcher notes these transitions and is trying to calculate the average time spent in each area by the birds.

If the park is designed in such a way (irreducible) that a bird can eventually get from any area to any other area, and there's no set pattern to their movement (aperiodic), then over a long period of observation (as \( T \) approaches infinity), the birdwatcher will notice that the proportion of time the birds spend in each area will settle down to a stable pattern. This stable pattern represents the true distribution of where the birds spend their time.

The Ergodic Theorem tells us that the birdwatcher's long-term observations (the average time the birds are noted in each area) will almost surely converge to the true, underlying distribution of the bird's presence across the park, no matter where the birds started from. This is the essence of what ergodicity means in this context: long-term observations reflect the true characteristics of the system.
\newline
\textbf{Mathematical Explanation}
\newline
Mathematically, the Ergodic Theorem is a formal statement about the long-term behavior of Markov chains. Here's a breakdown:
\begin{itemize}
    \item \textbf{Irreducible}: The Markov chain can go from any state to any other state in a finite number of steps.
    \item \textbf{Aperiodic}: The system doesn't operate in a fixed cycle; transitions between states can happen at any time.
    \item \textbf{Finite State Space \( \Omega :\)} There's a limited number of states that the system can be in.
    \item \textbf{Detailed Balance}: Each transition between states is in a sort of equilibrium with its reverse transition, weighted by the stationary distribution \( p \).
    \item \textbf{Bounded Function}: \( f \) The function \( f \) gives us a numeric value for each state, and its values are limited within a certain range.
\end{itemize}


Theorem states that for such a Markov chain, the time average of function \( f \) over the states \( X_t \) that the chain visits (denoted \( \hat{f}_T \)) will converge almost surely (which means with probability 1) to the expected value of \( f \) under the stationary distribution \( p \) as time \( T \) goes to infinity.

In other words, if you observe the system for a long time and calculate the average value of some property (given by the function \( f \)) of the states visited by the Markov chain, this average will eventually be the same as the average value of that property if you were to sample from the states according to their true long-term probabilities \( p \).

The "almost surely" part is a technical term in probability theory that means the convergence will happen with certainty, although there might be some rare and exceptional sample paths for which it does not happen.

This theorem is critical because it justifies using long Markov chain simulations to estimate averages that are otherwise difficult to compute, especially in complex systems like those found in statistical physics, economics, and many fields of engineering.
\newline
The more general statement covering continuous target distributions asks for a positive or Harris recurrent chain. The conditions are simpler here because we are assuming a finite state space for the Markov chain (not just countable).
We would really like to have a CLT for $\hat{f}_n$ formed from the Markov chain output, so we have confidence intervals $\pm \sqrt{\operatorname{var}\left(\hat{f}_n\right)}$ as well as the central point estimate $\hat{f}_n$ itself. CLT's hold for all the examples in this course. 


\section{Discrete-time Stochastic Processes}
A discrete-time $\mathbb{X}$-valued stochastic process is a process where, for each $t \in \mathbb{N}, X_{t}$ is a random variable taking values in some space $\mathbb{X}$. Typically, we will deal with either discrete spaces (such as a finite set like $\{1,2, \ldots, d\}$ for some $d \in \mathbb{N}$, or a countable set, like the set of integers $\mathbb{Z}$ ), or continuous spaces (such as $\mathbb{R}$ or $\mathbb{R}^{d}$ for some $d \in \mathbb{N}$ ). The space $\mathbb{X}$ is often called the state space. In order to characterize a discrete-time stochastic process, it is sufficient to know all of its finite dimensional distributions, that is, the joint distributions of the process at any collection of finitely many times. For a collection of times $\left(t_{1}, \ldots, t_{n}\right)$ and a collection of measurable sets of $\mathbb{X},\left(A_{t_{1}}, \ldots, A_{t_{n}}\right)$, the process is associated with the joint probability

$$
\mathbb{P}\left(X_{t_{1}} \in A_{t_{1}}, X_{t_{2}} \in A_{t_{2}}, \ldots, X_{t_{n}} \in A_{t_{n}}\right)
$$

We will focus here on the class of stochastic processes called "Markov", which are useful in the context of Monte Carlo methods. We will see that their specification only requires an "initial distribution" and a "transition probability" or "transition kernel", both of which are conceptually simple objects.

\section{Discrete State Space Markov Chains}
\subsection{Markov property}
Let us first consider discrete state spaces, i.e. $|\mathbb{X}|$ is finite or countably infinite. We can assume, without loss of generality, that the state space $\mathbb{X}$ is $\mathbb{N}$. In this context, we can work with the probability of the process taking a particular value at a particular time $t$. . For any $t \in \mathbb{N}$, we always have the following decomposition, for a collection of points $\left(x_{1}, \ldots, x_{t}\right)$ in $\mathbb{X}$,

$$
\begin{aligned}
\mathbb{P}\left(X_{1}\right. & \left.=x_{1}, X_{2}=x_{2}, \ldots, X_{t}=x_{t}\right) \\
& =\mathbb{P}\left(X_{1}=x_{1}, X_{2}=x_{2}, \ldots, X_{t-1}=x_{t-1}\right) \mathbb{P}\left(X_{t}=x_{t} \mid X_{1}=x_{1}, X_{2}=x_{2}, \ldots, X_{t-1}=x_{t-1}\right) \\
& =\mathbb{P}\left(X_{1}=x_{1}\right) \prod_{s=2}^{t} \mathbb{P}\left(X_{s}=x_{s} \mid X_{1}=x_{1}, \ldots, X_{s-1}=x_{s-1}\right)
\end{aligned}
$$

From this decomposition, we can construct all of the finite dimensional distributions using simply the sum and product rules of probability. To simplify the process, we can assume that the distribution of $X_{s}$ given its "past" $\left(X_{1}, \ldots, X_{s-1}\right)$ depends only upon $X_{s-1}$; i.e. we have

$$
\mathbb{P}\left(X_{s}=x_{s} \mid X_{1}=x_{1}, \ldots, X_{s-1}=x_{s-1}\right)=\mathbb{P}\left(X_{s}=x_{s} \mid X_{s-1}=x_{s-1}\right) .
$$

The fact that $X_{t}$ depends only on $X_{t-1}$ is often called the "Markov property".

When dealing with discrete state spaces, it is often convenient to associate each probability distribution with a row vector, with non-negative entries and summing to one. Now, given a random variable $X$ on $\mathbb{X}$, we say that $X$ has distribution $\mu$ for some vector $\mu$ (with non-negative entries and summing to one), and we note:

$$
\forall x \in \mathbb{X} \quad \mathbb{P}(X=x)=\mu(x)
$$

\subsection{Homogeneous Markov Chains}
Markov chains are called homogeneous when the conditional probabilities that do not depend on the time index, i.e.

$$
\forall x, y \in \mathbb{X} \quad \forall t, s \in \mathbb{N} \quad \mathbb{P}\left(X_{t}=y \mid X_{t-1}=x\right)=\mathbb{P}\left(X_{t+s}=y \mid X_{t+s-1}=x\right)
$$

In this setting, we can introduce the transition matrix $K(i, j)=K_{i j}=\mathbb{P}\left(X_{t}=j \mid X_{t-1}=i\right) . K$ is also referred to as the kernel of the Markov chain. If we call $\mu_{t}$ the distribution of $X_{t}, \mu_{t}(i)=\mathbb{P}\left(X_{t}=i\right)$, then by combining (11)-(2)-(3), the joint distribution of the chain over any finite time steps satisfies

$$
\mathbb{P}\left(X_{1}=x_{1}, X_{2}=x_{2}, \ldots, X_{t}=x_{t}\right)=\mu_{1}\left(x_{1}\right) \prod_{s=2}^{t} K_{x_{s-1} x_{s}}
$$

\subsection{Transition Matrix}
Define $K^{n}$ with entries $K^{n}(i, j)$, the matrix of transition from $i$ to $j$ in $n$ steps:

$$
K_{i j}^{n}=\mathbb{P}\left(X_{t+n}=j \mid X_{t-1}=i\right) .
$$

 \subsection{Chapman-Kolmogorov equation}

$$
K_{i j}^{m+n}=\sum_{k \in \mathbb{X}} K_{i k}^{m} K_{k j}^{n} .
$$

which proves that indeed $K^{n}$ is the $n^{\text {th }}$ matrix power of $K$, and hence the notation is consistent with standard linear algebra. For the marginal laws of $X_{t}$, we obtain the expression

$$
\mu_{t+1}(j)=\sum_{i \in \mathbb{X}} \mu_{t}(i) K_{i j}
$$

If $\mathbb{X}$ is finite, this is nothing else than a standard vector-matrix multiplication, hence we rewrite the equation as

$$
\mu_{t+1}=\mu_{t} K
$$

Similarly, we obtain $\mu_{t+n}=\mu_{t} K^{n}$.

\subsection{Summary}
\begin{itemize}
    \item The distribution $\mu_{0}$ of $X_{0}$, called the "initial distribution", must be specified
    \item Transition kernel $K$ must be specified, and it characterizes the law of $X_{t}$ given $X_{t-1}$, at any time $t$
    \item The distribution $\mu_{0}$ and the transition $K$ completely define the Markov chain $\left(X_{t}\right)$, using the Chapman-Kolmogorov equation above and the fact that finite-dimensional joint distributions characterize stochastic processes, i.e. Kolmogorov extension theorem.
\end{itemize}

\section{Continuous State Space Markov Chains}
\subsection{From discrete to continuous spaces}
The concepts of reversibility and detailed balance are essentially unchanged from the discrete setting. It is necessary to consider integrals with respect to densities rather than sums over probability distributions, but no fundamental differences arise here. For instance we can introduce reversibility as follows.


When facing with continuous state spaces, the main difficulty stems from the fact that the probability of any continuous random variable taking any particular value is zero. For example, if a random variable $X$ follows normal distribution, then for any $x \in \mathbb{R}, \mathbb{P}(X=\{x\})=0$. Hence, we cannot for instance refer to transition probabilities $\mathbb{P}\left(X_{t}=\{y\} \mid X_{t-1}=x\right)$. The Markov property (2) can still be defined on a continuous state space, as follows. We say that the process $\left(X_{t}\right)_{t \in \mathbb{N}}$ is a Markov chain if for any measurable set $A \subset \mathbb{X}:$

$$
\mathbb{P}\left(X_{t} \in A \mid X_{1}=x_{1}, X_{2}=x_{2}, \ldots, X_{t-1}=x_{t-1}\right)=\mathbb{P}\left(X_{t} \in A \mid X_{t-1}=x_{t-1}\right)
$$

It is often convenient to describe the distribution of a random variable $X$ over $\mathbb{X}$ in terms of some probability density function, $\mu: \mathbb{X} \rightarrow \mathbb{R}^{+}$which has the property that, if $X \sim \mu$, then we have for any measurable set $A$,

$$
\mathbb{P}(X \in A)=\int_{A} \mu(x) d x
$$

For homogeneous chains, we may describe the conditional probabilities of interest as a kernel function $K: \mathbb{X} \times \mathbb{X} \rightarrow \mathbb{R}$ which has the property that for all measurable sets $A \subset \mathbb{X}$ and all $x \in \mathbb{X}:$

$$
\mathbb{P}\left(X_{t} \in A \mid X_{t-1}=x\right)=\int_{A} K(x, y) d y:=K(x, A)
$$

that is conditional on $X_{t-1}=x, X_{t}$ is a random variable which admits a probability density function $y \mapsto K(x, y)$.

Hence for any collection of measurable sets $A_{1}, A_{2}, \ldots, A_{t}$ the following holds:

$$
\mathbb{P}\left(X_{1} \in A_{1}, X_{2} \in A_{2}, \ldots, X_{t} \in A_{t}\right)=\int_{A_{1} \times \cdots \times A_{t}} \mu\left(x_{1}\right) \prod_{k=2}^{t} K\left(x_{k-1}, x_{k}\right) d x_{1} \cdots d x_{t}
$$

We can also define the $m$-step conditional distributions,

$$
\mathbb{P}\left(X_{t+m} \in A \mid X_{t}=x_{t}\right)=\int_{\mathbb{X}^{m-1} \times A} \prod_{k=t+1}^{t+m} K\left(x_{k-1}, x_{k}\right) d x_{t+1} \cdots d x_{t+m}
$$

and it is useful to define an $m$-step transition kernel in the same manner as in the discrete case. Here matrix multiplication is replaced by a convolution operation but the intuition remains the same; i.e. we can rewrite the expression above as

$$
\mathbb{P}\left(X_{t+m} \in A \mid X_{t}=x_{t}\right)=\int_{A} K^{m}\left(x_{t}, x_{t+m}\right) d x_{t+m}=: K^{m}\left(x_{t}, A\right)
$$

where

$$
K^{m}\left(x_{t}, x_{t+m}\right)=\int_{\mathbb{X}^{m-1}} \prod_{k=t+1}^{t+m} K\left(x_{k-1}, x_{k}\right) d x_{t+1} \cdots d x_{t+m-1}
$$

Denoting by $\mu_{t}$ the density of the marginal distribution of $X_{t}$, we obtain

$$
\mu_{t+m}(y)=\int_{\mathbb{X}} \mu_{t}(x) K^{m}(x, y) d x
$$

and, in terms of sets,

$$
\mu_{t+m}(A)=\mathbb{P}\left(X_{t+m} \in A\right)=\int_{A} \int_{\mathbb{X}} \mu_{t}(x) K^{m}(x, y) d x d y
$$


\section{Important Properties}
\subsection{Definition Accessibility}
A state $y$ is accessible from a state $x$, written " $x \rightarrow y$ ", if

$$
\inf \left\{t: \mathbb{P}\left(X_{t}=y \mid X_{1}=x\right)>0\right\}<\infty
$$

Note that this can be rewritten equivalently as $\inf \left\{t: K_{x y}^{t}>0\right\}<\infty$. In layman's terms, $x \rightarrow y$ means that starting from $x$, there is a positive probability of reaching $y$ at some finite time in the future, according to the Markov kernel $K$.
\subsection{Definition Communication}
Two states $x, y \in \mathbb{X}$ are said to communicate if and only if $x \rightarrow y$ and $y \rightarrow x$.

These notions allow the study of the "communication structure" of a Markov chain, i.e. from which points it is possible to travel to, and back from. We now introduce a concept to describe the properties of the full state space, or significant parts of it, rather than individual states.
\subsection{Definition Irreducibility (Discrete)}
A Markov chain is said to be irreducible if all the states communicate, i.e. $\forall x, y \in \mathbb{X}: x \rightarrow y$. Given a probability distribution $\nu$ on $\mathbb{X}$, a Markov chain is said to be $\nu$-irreducible if every state with positive probability under $\nu$ communicates with every other such state:

$$
\forall x, y \in \operatorname{supp}(\nu): x \rightarrow y
$$

where $\operatorname{supp}(\nu)=\{x \in \mathbb{X}: \nu(x)>0\}$. A Markov chain is said to be strongly irreducible if any state can be reached from any other state, in only one step of the Markov chain. A Markov chain is said to be strongly $\nu$-irreducible if all states in supp $(\nu)$ may be reached in a single step from any other state in $\operatorname{supp}(\nu)$.

This notion is important for the study of Markov chain Monte Carlo methods: indeed a Markov chain that is $\nu$-irreducible can explore the entire support of $\nu$, rather than being restricted to a subset of it. Thus, when we will introduce Markov chains to explore a particular distribution of interest $\pi$, we will carefully check whether the chains are $\pi$-irreducible.

\subsection{Definition Irreducibility (Continuous)}
Given a distribution $\mu$ over $\mathbb{X}$, a Markov chain is said to be $\mu$ irreducible if, for all points $x \in \mathbb{X}$ and all measurable sets $A$ such that $\mu(A)>0$, there exists some $t$ such that:

$$
K^{t}(x, A)>0
$$

If this condition holds with $t=1$, then the chain is said to be strongly $\mu$-irreducible.

In practice, we will deal with $\pi$-irreducible Markov chains, where $\pi$ is the "target" distribution of interest. Periodicity for continuous space Markov chains can be introduced as follows.

\section{Properties of States}
\subsection{Periodicity (Discrete)}
For a Markov chain with kernel $K$, a state $x$ has period $d(x)$ defined as:

$$
d(x)=\operatorname{gcd}\left\{s \geq 1: K_{x x}^{s}>0\right\}
$$

where "gcd" denotes the greatest common denominator. If a chain induces a state $x$ of period $d(x)>1$, it is said to have a cycle of length $d(x)$.
\subsection{Periodicity (Continuous)}
A $\mu$-irreducible Markov chain with transition kernel $K$ is said to be periodic, if there exists some partition of the state space, $\mathbb{X}_{1}, \ldots, \mathbb{X}_{d}$ for $d \geq 2$, i.e. $\forall i \neq j: \mathbb{X}_{i} \cap \mathbb{X}_{j}=\varnothing$ and $\cup_{i=1}^{d} \mathbb{X}_{i}=\mathbb{X}$, with the property

$$
\forall i, j, t, s: \mathbb{P}\left(X_{t+s} \in \mathbb{X}_{j} \mid X_{t} \in \mathbb{X}_{i}\right)=\left\{\begin{array}{ll}
1 & j=i+s \bmod s \\
0 & \text { otherwise. }
\end{array} .\right.
$$

Otherwise the Markov chain is \textbf{aperiodic}.

A Markov chain with a period $d \geq 2$ is such that the chain moves with probability 1 from set $\mathbb{X}_{1}$ to $\mathbb{X}_{2}, \mathbb{X}_{2}$ to $\mathbb{X}_{3} \ldots$ and $\mathbb{X}_{d-1}$ to $\mathbb{X}_{1}$ and $\mathbb{X}_{d}$ to $\mathbb{X}_{1}$. Hence the chain will visit a particular element of the partition with a period $d$.

\subsection{Proposition 2.1.}
All states that communicate have the same period, therefore all states have the same period if the Markov chain is irreducible.

\subsection{Transience and Recurrence (Discrete)}
 For a Markov chain, a state $x$ is termed transient if:

$$
\mathbb{E}_{x}\left(\eta_{x}\right)<\infty
$$

Otherwise the state is called recurrent and

$$
\mathbb{E}_{x}\left(\eta_{x}\right)=\infty
$$

For irreducible Markov chains, transience and recurrence are properties of the chain itself, rather than its individual states. if any state is transient (or recurrent) then all states have that property.

\subsection{Transience and Recurrence (Continous)}
For a $\mu$-irreducible Markov chain, a set $A \subset \mathbb{X}$ is recurrent if

$$
\forall x \in A \quad \mathbb{E}_{x}\left(\eta_{A}\right)=\infty
$$

$A$ set $A \subset \mathbb{X}$ is uniformly transient if there exists some $M<\infty$ such that:

$$
\forall x \in A \quad \mathbb{E}_{x}\left(\eta_{A}\right) \leq M
$$

$A$ set $A \subset \mathbb{X}$ is transient if it may be expressed as a countable union of uniformly transient sets, i.e.:

$$
\exists\left\{M_{i} \in \mathbb{R}\right\} \quad \exists\left\{B_{i} \subset \mathbb{X}\right\}_{i=1}^{\infty}: A \subset \cup_{i=1}^{\infty} B_{i} \quad \forall i \in \mathbb{N} \quad \forall x \in B_{i} \quad \mathbb{E}_{x}\left(\eta_{B_{i}}\right) \leq M_{i}<\infty
$$

A Markov chain is recurrent if the following two conditions are satisfied:

\begin{itemize}
  \item the chain is $\mu$-irreducible for some distribution $\mu$;
  \item for every measurable set $A \subset \mathbb{X}$ such that $\mu(A)=\int_{A} \mu(x) d x>0, \mathbb{E}_{x}\left(\eta_{A}\right)=\infty$ for every $x \in A$.
\end{itemize}

The chain is transient if it is $\mu$-irreducible for some distribution $\mu$ and the entire space is transient.

\section{Equilibrium}
For Markov chain Monte Carlo methods, we will be particularly interested in Markov kernels admitting an invariant distribution.
\subsection{Invariant Distribution (Discrete)}
A distribution $\pi$ is said to be invariant or stationary for a Markov kernel, $K$, if $\pi K=\pi$.

The invariant distribution $\pi$ of a Markov kernel $K$ is thus simply the left eigenvector with unit eigenvalue. If there exists $t$ such that $X_{t} \sim \pi$ where $\pi$ is a stationary distribution, then $X_{t+s} \sim \pi K^{s}=\pi$ for all $s \in \mathbb{N}$, i.e. the Markov chain then keeps the same marginal distribution $\pi$ forever. A Markov chain is said to be in its stationary regime once this has occurred.
\textbf{The very remarkable property of \textbf{aperiodic}, \textbf{irreducible} Markov chains with an invariant distribution $\pi$, is that the marginal distribution of the chain $X_{t}$ converges to the invariant distribution $\pi$}. This is true for any starting state $x$ or starting distribution $\mu$. Hence the invariant distribution is also called the equilibrium distribution, or limiting distribution.

\subsection{Invariant Distribution (Continuous)}
 A probability distribution with density $\pi$ is said to be invariant or stationary for a Markov kernel $K$, if

$$
\forall y \in \mathbb{X} \quad \int_{\mathbb{X}} \pi(x) K(x, y) d x=\pi(y)
$$

A Markov kernel that admits an invariant probability distribution and that is $\mu$-irreducible for some $\mu$ is said to be positive recurrent. Then, it can be shown that the invariant probability distribution is unique. A slightly stronger form of recurrence is widely employed in the proof of many theoretical results which underlie Markov chain Monte Carlo. This form of recurrence is known as Harris recurrence.

 \subsection{Harris Recurrence}
 A set $A \subset \mathbb{X}$ is Harris recurrent if $\mathbb{P}_{x}\left(\eta_{A}=\infty\right)=1$ for every $x \in \mathbb{X}$. A Markov chain is Harris recurrent if it is $\mu$-irreducible and if every set $A$ such that $\mu(A)>0$ is Harris recurrent.

 
\subsection{Convergence to equilibrium}
Consider a Markov chain $\left(X_{t}\right)_{t \in \mathbb{N}}$ on a discrete space $\mathbb{X}$ that is aperiodic, irreducible and with an invariant distribution $\pi$. Let $\mu$ be any initial distribution on $\mathbb{X}$. Then

$$
\forall x \in \mathbb{X} \quad \mathbb{P}_{\mu}\left(X_{t}=x\right) \underset{t \rightarrow \infty}{ } \pi(x)
$$

\subsection{Detailed Balance (Discrete)}
A Markov kernel $K$ satisfies the detailed balance condition for a distribution $\pi$ if

$$
\forall x, y \in \mathbb{X} \quad \pi(x) K_{x y}=\pi(y) K_{y x}
$$
\subsection{Detailed Balance (Continous)}
A Markov kernel satisfies the so-called detailed balance condition for some distribution of density $\pi$, if

$$
\forall x, y \in \mathbb{X}: \pi(x) K(x, y)=\pi(y) K(y, x)
$$

The the following holds:

\begin{itemize}
  \item the distribution $\pi$ is invariant for the kernel $K$,
  \item the chain is reversible with respect to $\pi$.
\end{itemize}

\subsection{Detailed balance implies an invariant distribution}
If a Markov kernel $K$ satisfies the detailed balance condition for $\pi$, then $\pi$ is invariant for $K$.

\subsection{Time-reversal Markov chain}
Consider an irreducible Markov chain $\left(X_{t}\right)_{t \in \mathbb{N}}$, with kernel $K$ and invariant distribution $\pi$. Assume that $X_{1} \sim \pi$, i.e. the chain is started at stationarity. Define $Y_{t}=X_{T-t}$ for some $T \geq 1$. Then $\left(Y_{t}\right)_{0 \leq t \leq T}$ is an irreducible Markov chain, with invariant distribution $\pi$ and with initial distribution also $\pi$, and its transition kernel $\hat{K}$ is given by

$$
\forall x, y \in \mathbb{X} \quad \pi(x) K_{x y}=\pi(y) \hat{K}_{y x} .
$$

The chain $\left(Y_{t}\right)$ is called the time-reversal chain of $\left(X_{t}\right)$.

\subsection{Reversibility (Discrete}
A irreducible Markov chain $\left(X_{t}\right)_{t \in \mathbb{N}}$, with kernel $K$ and invariant distribution $\pi$, is said to be reversible (with respect to $\pi$ ) if its transition kernel $K$ is equal to the transition kernel $\hat{K}$ of its time-reversal chain, in other words, at stationarity we have

$$
\forall x, y \in \mathbb{X} \quad \mathbb{P}\left(X_{t}=x \mid X_{t+1}=y\right)=\mathbb{P}\left(X_{t}=x \mid X_{t-1}=y\right)
$$

Numerous Markov chains that we will study later on are reversible. Reversibility is typically verified by checking the detailed balance condition as discussed above.

\subsection{Reversibility (Continuous)}
A Markov kernel $K$ is reversible with respect to $\pi$ if

$$
\forall f: \mathcal{B}\left(\mathbb{X}^{2} \rightarrow \mathbb{R}\right) \quad \iint f(x, y) \pi(d x) K(x, d y)=\iint f(y, x) \pi(d x) K(x, y)
$$

where $\mathcal{B}\left(\mathbb{X}^{2} \rightarrow \mathbb{R}\right)$ refers to the bounded functions measurable functions $f$ from $\mathbb{X}^{2}$ to $\mathbb{R}$.


\subsection{Detailed balance implies reversibility}
If a Markov chain has a transition kernel $K$ satisfying the detailed balance condition for some distribution $\pi$ , then the chain is reversible with respect to $\pi$.

\section{Selected Convergence Results}
\subsection{A Simple Ergodic Theorem}
If $K$ is a $\pi$-irreducible $\mathbb{X}$-valued Markov kernel with stationary distribution $\pi$, then almost surely (i.e. with probability one), for any integrable function $\phi: \mathbb{X} \rightarrow \mathbb{R}:$

$$
\lim _{t \rightarrow \infty} \frac{1}{t} \sum_{i=1}^{t} \phi\left(X_{i}\right)=\int_{\mathbb{X}} \phi(x) \pi(x) d x
$$

for $\pi$-almost all starting values $x$ (i.e. for any $x$ except for some set $\mathcal{N}$ such that $\int_{\mathcal{N}} \pi(x) d x=0$ ).

\subsection{A Stronger Ergodic Theorem}
If $K$ is a $\pi$-invariant, Harris recurrent Markov chain with stationary distribution $\pi$, then almost surely, for any integrable function $\phi: \mathbb{X} \rightarrow \mathbb{R}$ :

$$
\lim _{t \rightarrow \infty} \frac{1}{t} \sum_{i=1}^{t} \phi\left(X_{i}\right)=\int_{\mathbb{X}} \phi(x) \pi(x) d x
$$

for all starting values $x$.

\subsection{Theorem 4.3}
 Suppose the kernel $K$ is $\pi$-irreducible, $\pi$-invariant and aperiodic. Then, we have

$$
\lim _{t \rightarrow \infty} \int_{\mathbb{X}}\left|K^{t}(x, y)-\pi(y)\right| d y=0
$$

for $\pi$-almost all starting value $x$.

Laws of large numbers for stochastic processes are also called "ergodic theorems", the most famous one being Birkhoff ergodic theorem, which generalizes the above theorems. Before stating a central limit theorem, we mention different definitions of ergodicity.

\subsection{Geometric Ergodicity}
A $\pi$-invariant, Harris recurrent Markov chain with stationary distribution $\pi$ is geometrically ergodic if there exists a real $\rho<1$ and a non-negative function $M: \mathbb{X} \rightarrow \mathbb{R}^{+}$, such that for all measurable set $A$,

$$
\left|K^{n}(x, A)-\pi(A)\right| \leq M(x) \rho^{n} .
$$

\textbf{Geometric ergodicity is thus about convergence to equilibrium}, with \textbf{a specific geometric rate}. The bound still depends on the starting point $x$. Uniform ergodicity is a stronger property.

\subsection{Uniform Ergodicity}
 A $\pi$-invariant, Harris recurrent Markov chain with stationary distribution $\pi$ is geometrically ergodic if there exists a real $\rho<1$ and a real $M>0$, such that for all measurable set $A$,

$$
\left|K^{n}(x, A)-\pi(A)\right| \leq M \rho^{n} .
$$

Under regularity conditions, given e.g. in Jones, 2004, it is possible to obtain a central limit theorem for the ergodic averages of a Harris recurrent, $\pi$-invariant Markov chain, and a function $\phi: \mathbb{X} \rightarrow \mathbb{R}$ which has at least two finite moments (depending upon the combination of regularity conditions assumed, it may be necessary to have a finite moment of order $2+\delta)$. It can be proved that every reversible, geometrically ergodic, stationary Markov chain satisfies a central limit theorem, which is why geometric ergodicity was introduced above as an intermediate concept.

\subsection{A Central Limit Theorem} 
For a Harris recurrent, $\pi$-invariant Markov chain, and a function $\phi: \mathbb{X} \rightarrow \mathbb{R}$ satisfying enough regularity conditions,

$$
\sqrt{t}\left[\frac{1}{t} \sum_{i=1}^{t} \phi\left(X_{i}\right)-\int_{\mathbb{X}} \phi(x) \pi(x) d x\right] \underset{t \rightarrow \infty}{D} \mathcal{N}\left(0, \sigma^{2}(\phi)\right),
$$

where

$$
\sigma^{2}(\phi)=\mathbb{V}\left[\phi\left(X_{1}\right)\right]+2 \sum_{k=2}^{\infty} \mathbb{C} o v\left[\phi\left(X_{1}\right), \phi\left(X_{k}\right)\right]
$$

The variance and covariance in the expression above are with respect to the distribution $\pi$ of the Markov chain in its stationary regime.




\end{document}