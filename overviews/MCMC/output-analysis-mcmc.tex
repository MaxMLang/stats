\documentclass{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\title{MCMC Output Analysis}
\date{January 2024}

\begin{document}


\section{MCMC and Sampling}

\begin{itemize}
    \item \textbf{MCMC:} Markov Chain Monte Carlo (MCMC) is a class of algorithms for sampling from probability distributions. It constructs a Markov chain that has the desired distribution as its equilibrium distribution.
    \item \textbf{Sampling:} The process of generating observations from a probability distribution.
\end{itemize}

\section{Concept of Burn-in}
Burn-in is a term used to describe the initial phase of an MCMC simulation. During this phase, the Markov chain is allowed to run for a certain number of steps without using its samples for statistical inference. The purpose of the burn-in period is to allow the chain to reach its stationary distribution, which is the target distribution from which we want to sample. The idea is that the initial samples may not be representative of the target distribution, especially if the starting point is far from the regions of high probability. By discarding these early samples, we hope to reduce the impact of the initial conditions on the inference.

\begin{enumerate}
    \item \textbf{Start of the Chain:} The initial state of an MCMC simulation might not represent the target distribution well, often being arbitrarily chosen.
    \item \textbf{Convergence Time:} The Markov chain requires time to "forget" its initial state and converge towards the equilibrium distribution.
    \item \textbf{Burn-in Period:} The initial samples, which are not representative of the target distribution, are known as "burn-in samples."
    \item \textbf{Discarding Samples:} These burn-in samples are typically discarded to improve the quality of the sampling.
\end{enumerate}

\subsection{Determining Burn-in Length}

\begin{itemize}
    \item \textbf{Not Fixed:} The length of the burn-in period is not universally fixed and is often determined empirically.
    \item \textbf{Diagnostic Tools:} Various tools are used to estimate when the chain has converged, thus determining the burn-in period length.
\end{itemize}

\subsection{What is  mixing?}
\textbf{Mixing}: Refers to how well the Markov chain explores the target distribution. A chain that mixes well is one that moves throughout the entire distribution space, ensuring that all regions are appropriately sampled. Good mixing is crucial for ensuring that the samples generated by the MCMC method are representative of the entire distribution. Mixing is influenced by factors such as the choice of proposal distribution, the structure of the target distribution, and the parameters of the MCMC algorithm.


There are several factors that influence mixing, including:

\begin{itemize}
    \item The shape of the target distribution
    \item The level of correlation between parameters
    \item The exact MCMC algorithm used
    \item The parameters of the “proposal” distribution (under certain algorithms)
    \item The type of proposal(s)
\end{itemize}
The first two factors are structural, in that they are a consequence of the mathematical model assumed, but the last three are algorithmic and can be modified by varying the MCMC approach.

\subsection{Why It Matters}

\begin{itemize}
    \item \textbf{Accuracy and Bias:} Including burn-in samples in analysis can bias results and give inaccurate estimates.
    \item \textbf{Efficiency:} Discarding burn-in samples is crucial for the efficiency and reliability of statistical estimates from MCMC methods.
\end{itemize}

\section{Effective Sample Size}
The Effective Sample Size (ESS) is a critical measure used to assess the quality of samples obtained from an MCMC method.

\begin{itemize}
    \item \textbf{Definition}: \textbf{ESS is the number of independent samples that would provide the same information as the correlated samples from the MCMC simulation.}
    \item \textbf{Importance}: It quantifies the amount of information contained in the correlated samples.
    \item \textbf{Calculation}: ESS takes into account the variance of the target distribution and the autocorrelation between samples.
\end{itemize}

\subsection{Autocorrelation and MCMC Efficiency}
Autocorrelation between MCMC samples reduces the amount of information each sample contributes. 

\begin{itemize}
    \item Definition: Autocorrelation measures the correlation of a signal with a delayed copy of itself as a function of delay.
    \item Impact: High autocorrelation indicates redundancy among samples and leads to a lower ESS.
\end{itemize}

\subsection{Computational Considerations}
MCMC algorithms with high statistical efficiency often require more computation time. Thus, it is important to consider the trade-off between the quality of the samples (ESS) and the computational time (S) required to generate them.

\begin{equation}
    \text{ESS} = n \times \text{Statistical Efficiency}
\end{equation}

\subsection{Efficiency Metrics}
The Effective Independent Samples per CPU second (\( \frac{ESS}{S} \)) is a key metric for comparing MCMC algorithms, taking into account both the ESS and computational cost.

\begin{equation}
    \frac{ESS}{S} = \frac{\text{Number of Effective Independent Samples}}{\text{CPU Time for } n \text{ Steps of MCMC}}
\end{equation}
\end{document}