\documentclass[11pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{geometry}
\usepackage{booktabs}
\usepackage{array}
\geometry{margin=1in}

\title{Connections Between Penalized Likelihood, Bayesian Inference, Random Effects, and Splines}
\author{}
\date{}

\begin{document}
\maketitle

\section{Overview}
This document establishes the connections between penalized likelihood estimation (ridge regression), Bayesian inference via Maximum A Posteriori (MAP) estimation, Empirical Bayes, random effects models, and penalized splines. We maintain consistent notation throughout and clarify the meaning of parameters in each context.

\section{Penalized Likelihood Estimation (Ridge Regression)}

Consider the linear model $\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}$, where $\boldsymbol{\epsilon} \sim N(\mathbf{0}, \sigma^2\mathbf{I})$.

The ridge regression estimate minimizes:
\begin{equation}
Q(\boldsymbol{\beta}) = ||\mathbf{y} - \mathbf{X}\boldsymbol{\beta}||^2 + \lambda||\boldsymbol{\beta}||^2
\end{equation}

where:
\begin{itemize}
    \item $\boldsymbol{\beta}$: vector of regression coefficients we are estimating
    \item $\lambda$: penalty parameter controlling shrinkage
    \item $\sigma^2$: residual variance
\end{itemize}

The ridge estimate is:
\begin{equation}
\hat{\boldsymbol{\beta}}_{\text{ridge}} = (\mathbf{X}^T\mathbf{X} + \lambda\mathbf{I})^{-1}\mathbf{X}^T\mathbf{y}
\end{equation}

\section{Connection to Bayesian Inference via MAP}

Now consider a Bayesian framework with:
\begin{align}
\mathbf{y}|\boldsymbol{\beta} &\sim N(\mathbf{X}\boldsymbol{\beta}, \sigma^2\mathbf{I}) \\
\boldsymbol{\beta} &\sim N(\mathbf{0}, \tau^2\mathbf{I})
\end{align}

where:
\begin{itemize}
    \item $\tau^2$: prior variance of the regression coefficients
    \item $\sigma^2$: residual variance (same as above)
\end{itemize}

The posterior distribution is:
\begin{equation}
p(\boldsymbol{\beta}|\mathbf{y}) \propto \exp\left\{-\frac{1}{2\sigma^2}||\mathbf{y} - \mathbf{X}\boldsymbol{\beta}||^2 - \frac{1}{2\tau^2}||\boldsymbol{\beta}||^2\right\}
\end{equation}

The MAP estimate maximizes this posterior, which is equivalent to minimizing:
\begin{equation}
\frac{1}{\sigma^2}||\mathbf{y} - \mathbf{X}\boldsymbol{\beta}||^2 + \frac{1}{\tau^2}||\boldsymbol{\beta}||^2
\end{equation}

\textbf{Key Connection:} Setting $\lambda = \frac{\sigma^2}{\tau^2}$ makes the MAP estimate identical to the ridge regression estimate.

\section{Empirical Bayes Approach}

In the Empirical Bayes framework, we estimate the hyperparameters $\tau^2$ and $\sigma^2$ from the data before computing the posterior. The marginal likelihood of $\mathbf{y}$ is:

\begin{equation}
p(\mathbf{y}|\tau^2, \sigma^2) = \int p(\mathbf{y}|\boldsymbol{\beta}, \sigma^2) p(\boldsymbol{\beta}|\tau^2) d\boldsymbol{\beta}
\end{equation}

This gives:
\begin{equation}
\mathbf{y} \sim N(\mathbf{0}, \sigma^2\mathbf{I} + \tau^2\mathbf{X}\mathbf{X}^T)
\end{equation}

The Empirical Bayes procedure:
\begin{enumerate}
    \item Estimate $\hat{\tau}^2$ and $\hat{\sigma}^2$ by maximizing the marginal likelihood (or REML)
    \item Use these estimates to compute $\hat{\lambda} = \frac{\hat{\sigma}^2}{\hat{\tau}^2}$
    \item Apply this data-driven $\hat{\lambda}$ in the ridge regression formula
\end{enumerate}

\textbf{Key insight:} Empirical Bayes provides a principled way to choose the penalty parameter $\lambda$ by estimating the variance components from the data itself, rather than using cross-validation or other tuning methods.

\section{Random Effects Model}

Consider a mixed model:
\begin{equation}
\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \mathbf{Z}\mathbf{u} + \boldsymbol{\epsilon}
\end{equation}

where:
\begin{align}
\mathbf{u} &\sim N(\mathbf{0}, \tau^2\mathbf{I}) \quad \text{(random effects)} \\
\boldsymbol{\epsilon} &\sim N(\mathbf{0}, \sigma^2\mathbf{I}) \quad \text{(residuals)}
\end{align}

In this context:
\begin{itemize}
    \item $\mathbf{u}$: vector of random effects we are estimating (e.g., group-specific deviations)
    \item $\tau^2$: variance of random effects (between-group variance)
    \item $\sigma^2$: residual variance (within-group variance)
\end{itemize}

The Best Linear Unbiased Predictor (BLUP) of $\mathbf{u}$ solves:
\begin{equation}
\min_{\mathbf{u}} \left\{\frac{1}{\sigma^2}||\mathbf{y} - \mathbf{X}\boldsymbol{\beta} - \mathbf{Z}\mathbf{u}||^2 + \frac{1}{\tau^2}||\mathbf{u}||^2\right\}
\end{equation}

This has the same form as ridge regression with $\lambda = \frac{\sigma^2}{\tau^2}$.

\textbf{Connection to Empirical Bayes:} Mixed model estimation typically uses REML or ML to estimate $\tau^2$ and $\sigma^2$, which is exactly the Empirical Bayes approach applied to random effects.

\section{Connection to Penalized Splines}

A penalized spline can be written as:
\begin{equation}
f(x) = \sum_{j=1}^p \beta_j b_j(x) + \sum_{k=1}^K u_k \psi_k(x)
\end{equation}

where:
\begin{itemize}
    \item $b_j(x)$: basis functions for the fixed part
    \item $\psi_k(x)$: basis functions for the smooth part
    \item $\boldsymbol{\beta}$: fixed coefficients
    \item $\mathbf{u}$: penalized coefficients treated as random effects
\end{itemize}

This can be recast as the mixed model above, where:
\begin{itemize}
    \item $\mathbf{X}$ contains evaluations of $b_j(x)$
    \item $\mathbf{Z}$ contains evaluations of $\psi_k(x)$
    \item $\tau^2$: controls smoothness (high $\tau^2$ allows more flexibility)
    \item $\sigma^2$: residual variance
\end{itemize}

\textbf{Intuition:} When $\tau^2$ is large relative to $\sigma^2$ (small $\lambda$), the spline can be ``wiggly'' to capture real variation. When $\tau^2$ is small (large $\lambda$), the spline is heavily penalized toward smoothness.

\section{Summary Table}

\begin{table}[h]
\centering
\begin{tabular}{@{}p{2.5cm}p{2.8cm}p{2.2cm}p{2.5cm}p{2.5cm}@{}}
\toprule
\textbf{Framework} & \textbf{What we estimate} & \textbf{$\lambda$} & \textbf{$\tau^2$ meaning} & \textbf{$\sigma^2$ meaning} \\
\midrule
Ridge Regression & Regression coefficients $\boldsymbol{\beta}$ & Fixed penalty & -- & Residual variance \\
\midrule
Bayesian MAP & Regression coefficients $\boldsymbol{\beta}$ & $\frac{\sigma^2}{\tau^2}$ & Prior variance of $\boldsymbol{\beta}$ & Residual variance \\
\midrule
Empirical Bayes & Regression coefficients $\boldsymbol{\beta}$ + hyperparameters & $\frac{\hat{\sigma}^2}{\hat{\tau}^2}$ (estimated) & Prior variance (estimated from data) & Residual variance (estimated from data) \\
\midrule
Random Effects & Random effects $\mathbf{u}$ & $\frac{\sigma^2}{\tau^2}$ & Between-group variance & Within-group variance \\
\midrule
Penalized Splines & Spline coefficients $\mathbf{u}$ & $\frac{\sigma^2}{\tau^2}$ & Smoothness parameter (flexibility) & Residual variance \\
\bottomrule
\end{tabular}
\caption{Connections between different frameworks through the penalty parameter $\lambda$}
\end{table}

\section{Key Insights}

\begin{enumerate}
    \item All five approaches involve minimizing a penalized sum of squares with the same mathematical form.
    \item The penalty parameter $\lambda = \frac{\sigma^2}{\tau^2}$ represents the ratio of noise variance to signal variance.
    \item Empirical Bayes provides a data-driven way to estimate this ratio, making it particularly useful when:
    \begin{itemize}
        \item The appropriate level of shrinkage is unknown a priori
        \item Cross-validation is computationally expensive
        \item A principled, automatic method is desired
    \end{itemize}
    \item In the spline context, this ratio controls the bias-variance tradeoff:
    \begin{itemize}
        \item Large $\tau^2$ (small $\lambda$): More flexibility, captures local variation
        \item Small $\tau^2$ (large $\lambda$): More smoothness, captures global trends
    \end{itemize}
    \item The random effects interpretation provides an intuitive understanding: we're estimating deviations that have their own variance structure.
    \item Empirical Bayes connects the frequentist (REML/ML estimation in mixed models) and Bayesian worlds by using the data to inform the prior.
\end{enumerate}

\subsection*{Connection to Bayesian Inference}

The GAMM formulation can be shown to have a direct Bayesian interpretation. The penalized log-likelihood:

\begin{equation}
\ell_p(\boldsymbol{\theta}) = \sum_{i=1}^{n} \left[ y_i \eta_i - \log(1 + e^{\eta_i}) \right] - \frac{1}{2}\boldsymbol{\alpha}^T \mathbf{S}_{\text{adapt}} \boldsymbol{\alpha} - \frac{\lambda_v}{2} \sum_{v=1}^{V} b_v^2
\end{equation}

corresponds to the log-posterior under the following prior specification:

\begin{equation}
p(\boldsymbol{\beta}, \boldsymbol{\alpha}, \mathbf{b} | \boldsymbol{\lambda}) \propto \exp\left(-\frac{1}{2}\boldsymbol{\alpha}^T \mathbf{S}_{\text{adapt}} \boldsymbol{\alpha} - \frac{\lambda_v}{2} \sum_{v=1}^{V} b_v^2\right)
\end{equation}

This implies:
\begin{itemize}
    \item $\boldsymbol{\beta} \propto 1$ (improper uniform prior on fixed effects)
    \item $\boldsymbol{\alpha} | \boldsymbol{\lambda} \sim \mathcal{N}(0, \mathbf{S}_{\text{adapt}}^{-})$ where $\mathbf{S}_{\text{adapt}}^{-}$ is the pseudoinverse
    \item $b_v | \lambda_v \sim \mathcal{N}(0, \lambda_v^{-1})$ independently for each village
\end{itemize}

The prior is improper because:
\begin{enumerate}
    \item The uniform prior on $\boldsymbol{\beta}$ does not integrate to a finite value over $\mathbb{R}^p$
    \item The penalty matrix $\mathbf{S}_{\text{adapt}}$ is rank-deficient, meaning some basis functions (typically those in the nullspace corresponding to polynomial terms) are unpenalized, leading to infinite variance components
\end{enumerate}

The empirical Bayes approach estimates $\boldsymbol{\lambda}$ via REML/ML, treating it as fixed, while full Bayes would place hyperpriors on $\boldsymbol{\lambda}$.

\end{document}