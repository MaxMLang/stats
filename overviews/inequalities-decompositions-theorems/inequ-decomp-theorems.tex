\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}

\title{Overview of important statistical inequalities, theorems, and decompositions}
\author{}
\date{}

\begin{document}
\tableofcontents
\maketitle
\section{Chebyshev's Inequality}

\subsection{Statement}
Chebyshev's inequality provides a bound on the probability that the value of a random variable with finite variance is a certain distance from its mean. Mathematically, for a random variable \( X \) with mean \( \mu \) and variance \( \sigma^2 \), and for any real number \( k > 0 \), the inequality is stated as:
\[ \Pr(|X - \mu| \geq k\sigma) \leq \frac{1}{k^2} \]

\subsection{Importance and Use Cases}
This inequality is crucial in probability theory and statistics as it applies to any probability distribution with a defined mean and variance, regardless of the distribution's shape. It's often used to prove the Weak Law of Large Numbers and is fundamental in fields such as economics, finance, and quality control where it's essential to measure how much variation can be expected from the mean.

\subsection{Intuition}
The intuition behind Chebyshev's inequality is that the probability of a random variable deviating significantly from its mean decreases as the deviation increases. It tells us that for a random variable, most of its values are concentrated around the mean, providing a way to quantify that concentration without knowing the exact distribution.

\subsection{History}
Chebyshev's inequality is named after the Russian mathematician Pafnuty Chebyshev. It was proven and popularized by him in the 19th century. While Chebyshev formalized and proved the inequality, it had been used implicitly by other mathematicians like Bienaymé before him. It forms a foundation for the probability theory that has been expanded upon by many statisticians and mathematicians since then.


Absolutely, here's a structured overview of Jensen's Inequality similar to the previous format for Chebyshev's Inequality.

\section{Cantelli's Inequality}

\subsection{Statement}
Cantelli's Inequality, also known as the one-sided Chebyshev Inequality, provides a bound on the probability that a real-valued random variable deviates from its mean in one direction, either above or below. For a random variable \(X\) with expected value \( \mathbb{E}[X] \) and variance \( \sigma^2 \), and for any \( \lambda > 0 \), the inequality is given as:
\[ \Pr(X - \mathbb{E}[X] \geq \lambda) \leq \frac{\sigma^2}{\sigma^2 + \lambda^2} \]
Similarly, the bound on the lower tail is:
\[ \Pr(X - \mathbb{E}[X] \leq -\lambda) \leq \frac{\sigma^2}{\sigma^2 + \lambda^2} \]

\subsection{Importance and Use Cases}
Cantelli's Inequality is particularly useful in the fields of finance, risk management, and other areas where one-sided bounds are required. It provides a more precise estimate than the two-sided Chebyshev's Inequality when we are interested in the probability of a random variable falling on one side of its mean.

\subsection{Intuition}
The inequality captures the intuition that large deviations from the mean in one direction are unlikely. Cantelli's Inequality quantifies this unlikelihood and gives a tighter bound compared to Chebyshev's Inequality, which considers deviations in both directions.

\subsection{History}
The inequality is often attributed to Francesco Paolo Cantelli, who published it in 1928. However, the origins can be traced back to Chebyshev's work of 1874. Cantelli's work provided an asymmetric version of Chebyshev's Inequality, thus it is sometimes referred to as the one-sided Chebyshev Inequality. Cantelli's Inequality is a notable example of how earlier mathematical concepts can be refined to yield more precise and tailored results.


\section{Jensen's Inequality}

\subsection{Statement}
Jensen's Inequality relates convex functions to expectations in probability and integrals in real analysis. For a probability space \((\Omega, \mathcal{A}, \mu)\), a convex function \(\varphi: \mathbb{R} \rightarrow \mathbb{R}\), and a random variable \(X\) that is integrable, the inequality is expressed as:
\[ \varphi \left( \int_{\Omega} X \, d\mu \right) \leq \int_{\Omega} \varphi(X) \, d\mu \]

Suppose $X$ is a random variable such that $\mathbb{P}(a \leqslant X \leqslant b)=1$. If $g: \mathbb{R} \longrightarrow \mathbb{R}$ is convex on $[a, b]$, then
$$
\mathbb{E} g(X) \geqslant g(\mathbb{E} X) .
$$

If $g$ is concave, then
$$
\mathbb{E} g(X) \leqslant g(\mathbb{E} X)
$$


\subsection{Importance and Use Cases}
This inequality is significant in various fields, including economics, risk management, and optimization. It is used to derive bounds on quantities of interest where convexity is present and is particularly important in the context of expected utility theory in economics, where it helps in understanding risk aversion. It's also a foundational tool in the theory of convex optimization.

\subsection{Intuition}
The intuition behind Jensen's Inequality is that for a convex function, the expected value of the function applied to a random variable is at least as large as the function of the expected value of that variable. It captures the idea that the "average" outcome is less than or equal to the outcome of the "average" input when the function is convex.

\subsection{History}
Jensen's Inequality is named after the Danish mathematician Johan Jensen. He provided the proof for this inequality in the early 20th century. It is an essential result in the field of convex analysis and has been used to establish other important statistical principles and theorems. Its simplicity and generality make it a powerful tool in both theoretical and applied mathematics.


Of course! Here's a structured overview of Markov's Inequality similar to the previous format for Chebyshev's and Jensen's Inequalities.

\section{Markov's Inequality}

\subsection{Statement}
Markov's Inequality provides a bound on the probability that a nonnegative random variable is greater than or equal to some positive value. For a nonnegative random variable \(X\) and any \(a > 0\), the inequality is stated as:
\[ P(X \geq a) \leq \frac{\mathbb{E}[X]}{a} \]

\subsection{Importance and Use Cases}
Markov's Inequality is a fundamental tool in probability theory and statistics. It is particularly useful when dealing with rare events and provides an upper bound on probabilities without requiring knowledge of the exact distribution of the random variable. It serves as the basis for other inequalities, such as Chebyshev's Inequality, and is widely used in algorithms and theoretical computer science.

\subsection{Intuition}
The core idea behind Markov's Inequality is that the probability of a nonnegative random variable taking on large values is limited by its expected value. The inequality essentially states that a random variable can't be "too large" too often, relative to its average.

\subsection{History}
Markov's Inequality is named after the Russian mathematician Andrey Markov, known for his work in probability theory and number theory. The inequality is one of the simplest and earliest results in the study of probabilities and expectations, and it underscores the basic property of nonnegative random variables in terms of their mean. Markov's contributions to probability theory extend beyond this inequality, but it remains one of the most widely recognized results associated with his name.


Certainly! Here's a structured overview of the Cauchy-Schwarz Inequality.

\section{Cauchy-Schwarz Inequality}

\subsection{Statement}
The Cauchy-Schwarz Inequality is a fundamental result in linear algebra stating that for all vectors \( u \) and \( v \) in an inner product space, the absolute square of the inner product is less than or equal to the product of the individual inner products of the vectors with themselves:
\[ | \langle u, v \rangle |^2 \leq \langle u, u \rangle \cdot \langle v, v \rangle \]
This inequality also extends to random variables \( X \) and \( Y \), implying that:
\[ (\mathbb{E}[XY])^2 \leq \mathbb{E}[X^2] \cdot \mathbb{E}[Y^2] \]
Equality holds if and only if one of the vectors is a scalar multiple of the other, or in the probabilistic form, if \( X \) and \( Y \) are linearly related.

\subsection{Importance and Use Cases}
This inequality is pivotal in various mathematical fields including analysis, probability theory, and statistics. It provides a bridge between angles and lengths in geometry and has implications for the correlation between random variables, error bounds in numerical methods, and the stability of solutions to differential equations.

\subsection{Intuition}
The essence of the Cauchy-Schwarz Inequality is that the "overlap" of two vectors (or the covariance between two random variables) cannot exceed the product of their individual "sizes" or variances. This concept is analogous to the fact that the projection of one vector onto another cannot be longer than the original vector itself.

\subsection{History}
The inequality is named after Augustin-Louis Cauchy and Hermann Schwarz. Cauchy was a French mathematician who made numerous contributions to mathematics, while Schwarz was a German mathematician known for his work in analysis. The inequality was initially formulated by Cauchy in 1821 and later generalized by Schwarz in the 1880s. It is a classic example of a result that has been independently discovered and refined by multiple mathematicians across different mathematical areas.


\section{Cholesky Decomposition}

\subsection{Statement}
The Cholesky Decomposition is a method of decomposing a Hermitian positive-definite matrix \( A \) into the product of a lower triangular matrix \( L \) and its conjugate transpose \( L^* \), such that \( A = LL^* \). For real matrices, which are symmetric and positive-definite, the decomposition simplifies to \( A = LL^T \), where \( L^T \) denotes the transpose of \( L \). This factorization is unique as long as the matrix is positive-definite.

\subsection{Importance and Use Cases}
The Cholesky Decomposition is critical in numerical analysis for solving systems of linear equations, optimizing computational efficiency, and ensuring numerical stability. It is particularly useful when dealing with covariance matrices in statistics, where the matrices are symmetric and positive-definite, making this decomposition method preferred over others for its numerical stability and speed.

\subsection{Intuition}
The intuitive idea behind the Cholesky Decomposition is that any "nice" positive-definite matrix can be thought of as a product of a lower triangular matrix and its transpose. This reflects a structured, stepwise buildup of the matrix, akin to constructing a building by first laying down a solid triangular foundation and then adding levels that mirror the base.

\subsection{Positive Semidefinite Matrices}
For positive semidefinite matrices, which can have zero eigenvalues, the Cholesky Decomposition still applies, but the diagonal entries of \( L \) can be zero, leading to potential non-uniqueness in the decomposition. However, uniqueness can be restored by either considering only the non-zero part of the matrix or by employing a permutation matrix to reorder the matrix into a unique lower triangular form with positive diagonal.

\subsection{History}
The decomposition is named after the French mathematician André-Louis Cholesky, who developed the algorithm in the early 20th century. Although he used it practically for geodetic surveys, it was only published posthumously. The method is a special case of the LU decomposition and has since become a standard technique in computational matrix algebra.

Certainly! Here's an overview of the Gauss-Markov Theorem, organized in the requested format.

\section{Gauss-Markov Theorem}

\subsection{Statement}
The Gauss-Markov Theorem is a fundamental result in statistics that addresses the properties of ordinary least squares (OLS) estimators in the context of linear regression models. It states that, under certain assumptions, the OLS estimators of the coefficients in a linear regression model are unbiased, efficient (having the smallest variance among the class of linear unbiased estimators), and have the smallest mean squared error (MSE) among all linear unbiased estimators. Mathematically, for a linear regression model \(Y = X\beta + \epsilon\), where \(Y\) is the dependent variable, \(X\) is the design matrix, \(\beta\) is the vector of parameters, and \(\epsilon\) is the error term with mean zero and constant variance, the OLS estimators \(\hat{\beta}\) are the Best Linear Unbiased Estimators (BLUE).

\subsection{Importance and Use Cases}
The Gauss-Markov Theorem is of paramount importance in econometrics, statistics, and various fields that rely on linear regression analysis. It justifies the use of OLS estimators as the best linear unbiased estimators and highlights their optimality properties. This theorem underpins the foundations of regression analysis, providing a solid statistical framework for parameter estimation in linear models.

\subsection{Intuition}
The intuition behind the Gauss-Markov Theorem is that, among all possible linear unbiased estimators, the OLS estimators have the smallest variance, making them the most precise estimators. This means that, on average, OLS estimates are not systematically biased, and they achieve the smallest spread (variance) around the true parameter values. This optimality is particularly valuable when dealing with limited and noisy data.

\subsection{Assumptions}
The Gauss-Markov Theorem assumes that the errors \(\epsilon\) in the linear regression model are independent, have constant variance (\(\text{Var}(\epsilon) = \sigma^2\)), and are normally distributed. Additionally, it assumes that there is no perfect multicollinearity among the predictor variables in \(X\), and that the model is correctly specified.

\subsection{History}
The theorem is named after Carl Friedrich Gauss and Andrey Markov, who made foundational contributions to probability theory and statistics. However, the formal statement and proof of the Gauss-Markov Theorem were developed by multiple statisticians, including Maurice G. Kendall, Norbert Wiener, and Ragnar Frisch, during the early 20th century. The theorem serves as a cornerstone of modern regression analysis and is a testament to the foundational work laid by these eminent statisticians.

Certainly! Here's an overview of the Slutsky's Theorem, organized in the requested format.

section{Slutsky's Theorem}

\subsection{Statement}
Slutsky's Theorem is a fundamental result in mathematical statistics and econometrics that describes the behavior of sequences of random variables and the convergence of probability distributions. It states that if a sequence of random variables \(X_n\) converges in distribution to a random variable \(X\), and another sequence of random variables \(Y_n\) converges in distribution to a constant \(c\), then the following holds:
\[ X_n + Y_n \xrightarrow{d} X + c \]
\[ X_nY_n \xrightarrow{d} cX \]
In other words, the sum and product of random variables converging in distribution converge in distribution to the sum and product of their respective limits.

\subsection{Importance and Use Cases}
Slutsky's Theorem is a fundamental tool in statistics and econometrics, especially in the context of the central limit theorem and limit theorems for sample means. It allows researchers to make inferences about the distributions of sums and products of random variables as sample sizes increase, making it essential in hypothesis testing, confidence intervals, and statistical inference.

\subsection{Intuition}
The intuition behind Slutsky's Theorem is that the limiting behavior of a sum or product of random variables is determined by the limiting behavior of the individual random variables involved. When one of the random variables converges to a constant, its effect on the sum or product is straightforward, and the limit can be determined from the limiting behavior of the other random variable.

\subsection{Assumptions}
Slutsky's Theorem assumes that the sequences of random variables \(X_n\) and \(Y_n\) converge in distribution. This means that the cumulative distribution functions of \(X_n\) and \(Y_n\) converge to the cumulative distribution functions of \(X\) and \(c\) (the constant), respectively, as \(n\) approaches infinity.

\subsection{History}
The theorem is named after Eugen Slutsky, a Russian economist and statistician, who contributed significantly to the development of statistical and econometric theory in the early 20th century. While Slutsky's Theorem has direct applications in econometrics, its principles are widely used in statistical theory and form the basis for understanding the behavior of random variables in large samples. It plays a crucial role in the development of statistical methods for analyzing data and drawing meaningful conclusions.

\section{Convergence Types}

\subsection{Convergence in Probability}

\subsubsection{Definition}
Convergence in probability, denoted as \(X_n \xrightarrow{\mathbb{P}} X\), occurs when, for any positive \(\epsilon\), the probability that the absolute difference between \(X_n\) and \(X\) is greater than \(\epsilon\) approaches zero as \(n\) goes to infinity. Formally, \(\lim_{n \to \infty} \mathbb{P}(|X_n - X| > \epsilon) = 0\) for all \(\epsilon > 0\).

\subsubsection{Intuition}
Convergence in probability implies that as the sample size \(n\) increases, the values of \(X_n\) become increasingly likely to be close to the value of \(X\). It characterizes the convergence of random variables in terms of their probability distributions.

\subsubsection{Relationship to the Different Types of Convergences}
Convergence in probability is a weaker form of convergence compared to almost sure convergence and convergence in the \(r\)-th moment. It is also distinct from convergence in distribution, which focuses on the limiting behavior of cumulative distribution functions.

\subsubsection{Implications}
Convergence in probability is often used in statistical inference and hypothesis testing. It ensures that as the sample size grows, the estimates of parameters based on the sample converge in probability to the true parameter values, making it a key concept in the theory of estimation and statistics.

\subsection{Almost Sure Convergence}

\subsubsection{Definition}
Almost sure convergence, denoted as \(X_n \xrightarrow{\text{a.s.}} X\), occurs when the probability of a sample point \(\omega\) in the sample space \(\Omega\) for which \(X_n(\omega)\) does not converge to \(X(\omega)\) as \(n\) approaches infinity is zero, i.e., \(\mathbb{P}(\{\omega \in \Omega \mid X_n(\omega) \not\rightarrow X(\omega) \text{ for } n \rightarrow \infty\}) = 0\).

\subsubsection{ Intuition}
Almost sure convergence guarantees that, with probability one, the values of \(X_n\) converge to the values of \(X\) for almost all sample points \(\omega\) in the sample space. It is a strong form of convergence that holds for the majority of sample outcomes.

\subsubsection{ Relationship to the Different Types of Convergences}
Almost sure convergence is the strongest form of convergence among the mentioned types. It implies convergence in probability and convergence in the \(r\)-th moment but is not equivalent to convergence in distribution.

\subsubsection{ Implications}
Almost sure convergence is often used in situations where we want to ensure that the convergence occurs with certainty for almost all sample points. It plays a crucial role in various areas of probability theory and statistics.

\subsection{Convergence in the \(r\)-th Moment}

\subsubsection{ Definition}
Convergence in the \(r\)-th moment, denoted as \(X_n \xrightarrow{r} X\), occurs when, for each positive integer \(r\), the expected value of the \(r\)-th power of \(|X_n|\) is finite for all \(n\), and the expected value of the \(r\)-th power of \(|X_n - X|\) approaches zero as \(n\) goes to infinity.

\subsubsection{ Intuition}
Convergence in the \(r\)-th moment ensures that the moments (or powers) of the random variables \(X_n\) converge to the moments of the random variable \(X\) as \(n\) increases. It is a strong form of convergence that considers the behavior of higher moments.

\subsubsection{ Relationship to the Different Types of Convergences}
Convergence in the \(r\)-th moment implies convergence in probability but does not necessarily imply almost sure convergence or convergence in distribution.

\subsubsection{ Implications}
Convergence in the \(r\)-th moment is often used in situations where we want to ensure that not only the means but also higher moments of random variables converge as the sample size increases. It has applications in statistics, particularly when dealing with moments, variances, and higher-order statistics.

\subsection{ Convergence in Distribution}

\subsubsection{ Definition}
Convergence in distribution, denoted as \(X_n \xrightarrow{\mathcal{B}} X\), occurs when the cumulative distribution functions (CDFs) of \(X_n\) converge pointwise to the CDF of \(X\) at all continuity points of the limiting distribution. Formally, \(\lim_{n \to \infty} \mathbb{P}(X_n \leq x) = \mathbb{P}(X \leq x)\) for all \(x\) where \(F_X(x) = \mathbb{P}(X \leq x)\) is continuous.

\subsubsection{ Intuition}
Convergence in distribution describes the limiting behavior of the probability distribution of \(X_n\) as \(n\) goes to infinity. It focuses on the convergence of CDFs and characterizes how the distribution of \(X_n\) approaches the distribution of \(X\) as sample size increases.

\subsubsection{ Relationship to the Different Types of Convergences}
Convergence in distribution is distinct from the other types of convergences mentioned. It does not imply convergence in probability, almost sure convergence, or convergence in the \(r\)-th moment.

\subsubsection{ Implications}
Convergence in distribution is essential in the central limit theorem and is often used when dealing with large sample approximations. It helps analyze how the distribution of sample statistics approaches a limiting distribution as sample size increases.


\section{Glivenko–Cantelli Theorem}

\subsection{Statement}
The Glivenko–Cantelli theorem, also known as the Fundamental Theorem of Statistics, states that for a sequence of independent and identically distributed random variables, the empirical distribution function converges uniformly to the true cumulative distribution function. Formally, if \( X_1, X_2, \ldots \) are i.i.d. random variables with the common cumulative distribution function \( F(x) \), then the empirical distribution function \( F_n(x) \) defined as
$$
F_n(x)=\frac{1}{n} \sum_{i=1}^n I_{\left[X_i, \infty\right)}(x)=\frac{1}{n}\left|\left\{i \mid X_i \leq x, 1 \leq i \leq n\right\}\right|
$$
converges uniformly to \( F(x) \) almost surely, where \( \mathbb{1} \) is the indicator function.

 $$\left\|F_n-F\right\|_{\infty}=\sup _{x \in \mathbb{R}}\left|F_n(x)-F(x)\right| \longrightarrow 0 \text{ almost surely}$$

\subsection{Importance and Use Cases}
This theorem is pivotal in the field of statistics and has applications in machine learning and econometrics, particularly with M-estimators. It provides a guarantee for the consistency of empirical estimators, which is a cornerstone concept in statistics, ensuring that as more data is collected, the empirical results will converge to the true underlying probabilities.

\subsection{Intuition}
The intuition behind the Glivenko–Cantelli theorem is that as the sample size increases, the empirical distribution, constructed from observed data, will closely approximate the actual distribution from which the data is drawn. This convergence is uniform, meaning it applies across the entire range of the distribution function.

\subsection{History}
The theorem was established by Valery Glivenko and Francesco Paolo Cantelli in 1933. It bolstered the law of large numbers by providing a strong form of convergence, namely uniform convergence, for the empirical distribution function to the true distribution function. This result is fundamental in both theoretical and applied statistical work, and it underpins the reliability of empirical research findings based on large data sets.


\section{Central Limit Theorems}
\subsection{Lindeberg–Lévy Central Limit Theorem (CLT)}
\subsubsection{Statement}
The Lindeberg–Lévy Central Limit Theorem (CLT) asserts that if \( X_1, X_2, X_3, \ldots \) is a sequence of independent and identically distributed (i.i.d.) random variables with a common expected value \( E[X_i] = \mu \) and variance \( Var[X_i] = \sigma^2 < \infty \), then the normalized sum of these variables converges in distribution to a normal distribution. Mathematically, it states that as the sample size \( n \) approaches infinity, the standardized random variables \( \sqrt{n}(\bar{X}_n - \mu) \) converge in distribution to a standard normal distribution \( \mathcal{N}(0, \sigma^2) \), where \( \bar{X}_n \) is the sample mean of the first \( n \) variables.

$\sqrt{n}\left(\bar{X}_n-\mu\right) \xrightarrow{a} \mathcal{N}\left(0, \sigma^2\right)$.

\subsubsection{Importance and Use Cases}
The Lindeberg–Lévy CLT is a fundamental theorem in probability and statistics, as it explains why normal distributions arise so commonly in practice, even when the underlying distribution of individual observations is not normal. It is used in hypothesis testing, confidence interval estimation, and in various applications across natural and social sciences where the normal approximation to other distributions is required.
\subsubsection{Intuition}
The essence of the CLT is that when independent random effects are added together, their normalized sum tends to form a normal distribution, regardless of the shape of their original distribution. This can be thought of as the "democratizing effect" of aggregation: individual peculiarities wash out, and the aggregate effect is a bell curve, which is robust to the original data's distribution.

\subsubsection{History}
The theorem was first introduced by the Swedish mathematician Jarl Waldemar Lindeberg and later refined by the French mathematician Paul Lévy, which is why it carries their names.

\subsection{Weak Law of Large Numbers (WLLN)}

\subsubsection{Statement}
The Weak Law of Large Numbers states that for a sequence of independent and identically distributed (i.i.d.) random variables \( X_1, X_2, \ldots, X_n \) with a common expected value \( \mu \) and finite variance, the sample average \( \bar{X}_n = \frac{1}{n}\sum_{i=1}^{n}X_i \) converges in probability to \( \mu \) as \( n \) approaches infinity. Mathematically, it is expressed as:
\[ P\left(\left|\bar{X}_n - \mu\right| > \varepsilon\right) \rightarrow 0 \text{ as } n \rightarrow \infty, \]
for any \( \varepsilon > 0 \). 

\subsubsection{Importance and Use Cases}
The WLLN justifies the practice of estimating population parameters by sample means in statistics. It is widely used in fields such as economics, finance, and insurance to make predictions and understand long-term averages.

\subsubsection{Intuition}
The key intuition behind the WLLN is that fluctuations in the average of a large number of i.i.d. random variables tend to cancel out, making the average close to the expected value with high probability. It captures the concept that, while individual observations may vary widely, their average will likely be close to the population mean as the sample size grows.

\subsubsection{History}
The concept of the law of large numbers dates back to the work of Jacob Bernoulli in the late 17th century, with formalization and proof provided in the early 18th century. The term "weak" refers to the type of convergence it describes, which is convergence in probability, as opposed to almost sure convergence described by the Strong Law of Large Numbers.

\subsection{Strong Law of Large Numbers (SLLN)}

\subsubsection{Statement}
The Strong Law of Large Numbers extends the idea of the WLLN by stating that, under the same assumptions of i.i.d. random variables with common expected value \( \mu \), the sample average \( \bar{X}_n \) converges almost surely (or with probability 1) to \( \mu \). This is denoted as:
\[ P\left(\lim_{n \rightarrow \infty} \bar{X}_n = \mu\right) = 1. \]

\subsubsection{Importance and Use Cases}
The SLLN provides a stronger assurance than the WLLN for the convergence of the sample mean to the population mean, which is crucial for the theoretical foundation of many statistical methods, including parameter estimation and hypothesis testing.

\subsubsection{Intuition}
The SLLN suggests that not only does the probability of the sample mean deviating from the population mean go to zero, but also, with an infinite amount of data, the sample mean will equal the population mean with certainty. This law underlines the predictability of outcomes over the long run.

\subsubsection{History}
Building upon earlier work, the SLLN was first fully developed by Russian mathematician Andrey Kolmogorov in the 20th century. The "strong" in its name reflects the mode of convergence, which is almost sure convergence, indicating a stronger form of convergence compared to the WLLN's convergence in probability.

\subsection{Difference between WLLN and SLLN}

\subsubsection{Weak Law of Large Numbers (WLLN)}
The WLLN tells us that as we take more and more observations, the probability that our sample average will be close to the population mean gets higher and higher. However, it doesn't guarantee that it will always converge. In other words, there's still a non-zero chance, no matter how small, that our sample average could be far from the population mean, even with a large number of observations. The convergence is in probability, which means that if we were to repeat our sampling process over and over, most of our calculated sample averages would be close to the population mean, but we could still have some averages that are not.

Imagine you're throwing darts at a dartboard, and each throw represents an observation. The WLLN suggests that as you throw more darts, the average position of the darts will probably get closer to the bullseye, but some throws might still be off-target.

\subsubsection{Strong Law of Large Numbers (SLLN)}
The SLLN takes this a step further and says that the sample average will not just probably, but certainly (with probability 1), be equal to the population mean as the number of observations goes to infinity. This is a stronger statement because it tells us that the average of the observations will eventually converge to the population mean, and not just get close to it with high probability. This convergence is almost sure, meaning that if we were to observe the sampling process infinitely, the proportion of sample averages that differ from the population mean by any given amount would eventually shrink to zero.

Using the dart analogy again, the SLLN would be like saying that if you could throw an infinite number of darts, the average position of all your throws would be exactly the bullseye, not just close to it.

\subsubsection{In Summary}
\begin{itemize}
    \item \textbf{WLLN}: High probability of the sample mean being close to the population mean with a \textit{large but finite} number of samples.
    \item \textbf{SLLN}: Certainty that the sample mean will equal the population mean with an \textit{infinite} number of samples.
\end{itemize}

\section{Hammersley–Clifford Theorem}

\subsection{Statement}
The Hammersley–Clifford Theorem is a foundational result in the field of graphical models and probability theory. It states that a probability distribution obeys the Markov properties with respect to an undirected graph if and only if it can be represented as a product of potential functions that are defined over the cliques of the graph. Mathematically,  for a probability distribution \( p \) over a set of random variables \( X_V \), where \( V \) is the set of all vertices in the decomposable graph \( G \), and the graph \( G \) has cliques \( C_1, \ldots, C_k \), the distribution \( p \) is Markov with respect to \( G \) if and only if it can be expressed as a product of functions \( \psi(x_{C_i} | x_{S_i}) \) over each clique \( C_i \) conditioned on the separator set \( S_i \), normalized by the marginal probabilities of the separator sets. Mathematically:

\[ p(x_V) = \frac{1}{Z} \prod_{i=1}^{k} \psi(x_{C_i} | x_{S_i}), \]

where \( \psi \) are potential functions associated with each clique \( C_i \) conditioned on the separator set \( S_i \), \( x_{C_i} \) and \( x_{S_i} \) are the variables in clique \( C_i \) and separator set \( S_i \) respectively, and \( Z \) is a normalization constant ensuring that \( p \) sums to 1 over all possible values of \( x_V \).

The overall probability distribution \( p(x_V) \) is then the product of these conditioned clique potentials normalized by the marginals of the separators:

\[ p(x_V) = \prod_{i=1}^{k} \frac{p(x_{C_i})}{p(x_{S_i})}. \]

Here, the normalization constant \( Z \) is implicitly included by the division with the marginals of the separator sets \( p(x_{S_i}) \).

\subsection{(More) Probabilistic Statement}
\subsubsection{Joint Distribution}

\begin{itemize}
  \item \textbf{Joint Distribution}: This is a probability distribution that models two or more random variables simultaneously. For example, $\pi(x_1, x_2, \ldots, x_d)$ is a joint distribution for $d$ variables.
  \item \textbf{Positivity Condition}: The theorem requires the joint distribution to be strictly positive on its support. This means that for every possible combination of values that the random variables can take, the probability is greater than zero.
\end{itemize}

\subsubsection{Conditional Distributions and Independence}

\begin{itemize}
  \item \textbf{Conditional Distribution}: $\pi(X_j \mid X_{-j})$ is the probability distribution of a variable $X_j$ given all other variables. It reflects how the probability of $X_j$ changes when the values of the other variables are known.
  \item \textbf{Factorization}: The ability to represent the joint distribution as a product of conditional distributions simplifies complex dependencies into more manageable pairwise interactions.
\end{itemize}

\subsubsection*{Hammersley-Clifford Theorem and Independence Structure}

\begin{itemize}
  \item \textbf{Local Interactions}: In Markov random fields, the assumption is that each variable directly interacts only with its neighbors in the graph. These local interactions can describe the global distribution.
  \item \textbf{Factorization According to the Graph}: The theorem says that if the graph is undirected and the positivity condition is met, the joint distribution of the entire system can be broken down into a product of factors, each involving only local neighbors.
\end{itemize}

\subsubsection{Implications}

\begin{itemize}
  \item \textbf{Simplification}: Instead of calculating a complex multidimensional joint distribution, one can work with simpler conditional distributions, which is computationally more feasible.
  \item \textbf{Gibbs Sampling}: This is a technique used for sampling from a probability distribution. The theorem's factorization allows Gibbs sampling to update one variable at a time based on the conditional distributions, eventually converging to the joint distribution.
  \item \textbf{Independence and Correlations}: Even though variables may be correlated, their complete joint behavior is captured by the conditions of other variables, reflecting that correlations can be understood through local conditional dependencies.
\end{itemize}

\subsubsection{Practical Example}

Consider a social network where nodes are people and edges represent friendships. The joint distribution might represent the probability of a certain configuration of opinions within the network. The Hammersley-Clifford theorem suggests that to understand the distribution of opinions across the entire network, you only need to understand the conditional probabilities of each person's opinion given their friends' opinions, not the entire network's state. This reflects how people are influenced by their friends rather than by distant connections in the network.




\subsection{Importance and Use Cases}
This theorem is crucial for understanding the structure and factorization properties of probabilistic graphical models, such as Markov random fields (MRFs) and Bayesian networks. It provides a theoretical basis for the decomposition of complex joint distributions into simpler, local components, facilitating efficient computation of probabilities and inference. Applications include image processing, statistical physics, spatial statistics, and machine learning, where modeling complex dependencies between variables is essential.

\subsection{Intuition}
The intuition behind the Hammersley–Clifford Theorem is that the dependencies between variables in a probabilistic model can be captured by the graph structure, with edges representing direct interactions. The theorem formalizes the idea that the joint distribution of a set of variables with respect to a graph can be broken down into factors that depend only on subsets of variables that form cliques. This decomposition aligns with the notion that the global properties of a system can be understood through its local interactions.

\subsection{History}
The theorem is named after John Hammersley and Peter Clifford, who formulated and proved it in the 1970s. It built upon earlier work in statistical physics and graph theory, bridging these fields by formalizing the relationship between graph structures and probability distributions. The theorem has since become a cornerstone in the study of graphical models, underpinning much of the modern theoretical and applied work in areas requiring the representation and analysis of complex probabilistic relationships.

\section{De Finetti's Theorem}

\subsection{Statement}
De Finetti's Theorem is a fundamental result in the field of probability and statistics that applies to sequences of exchangeable random variables—a sequence where the joint probability distribution does not change when the order of the variables is altered. The theorem asserts that any such finite exchangeable sequence is conditionally independent given some latent variable. For infinite sequences, the theorem states that they can be represented as a mixture of i.i.d. sequences, implying that the original exchangeable sequence behaves as if it were a mixture of identical and independent processes, each with its own distribution drawn from a common "mixing" distribution.

\subsubsection{De Finetti's Theorem (Infinite Case)}

For an infinite sequence of exchangeable random variables \( (X_1, X_2, X_3, ...) \), there exists a probability measure \( \mu \) on the space of probability measures such that the joint probability distribution of the infinite sequence is a mixture of i.i.d. sequences, each with its own distribution. This means that the joint distribution \( P \) of \( (X_1, X_2, X_3, ...) \) can be represented as:

\[ P(X_1 \in A_1, ..., X_n \in A_n) = \int_{\mathcal{P}} \prod_{i=1}^n P_i(A_i) \, d\mu(P_1, P_2, ...), \]

for all \( n \) and all measurable sets \( A_1, ..., A_n \), where \( \mathcal{P} \) is the space of all probability measures and \( P_i \) is the marginal distribution of \( X_i \).

\subsubsection{De Finetti's Theorem (Finite Case)}

For a finite sequence of exchangeable random variables \( (X_1, X_2, ..., X_n) \), the joint distribution can be expressed in a similar manner as a mixture of i.i.d. sequences, but the representation is over the space of probability measures that could have generated the finite sequence.

The theorem essentially states that exchangeable random variables are conditionally independent and identically distributed given some latent variable \( \theta \), often interpreted as a parameter of the underlying process. The measure \( \mu \) represents our uncertainty about \( \theta \), and once \( \theta \) is known, the random variables are i.i.d. with the distribution \( P_{\theta} \).


\subsection{Importance and Use Cases}
De Finetti's Theorem is crucial for understanding the behavior of exchangeable events and for modeling in Bayesian statistics, where exchangeability is often assumed. It provides a theoretical basis for the concept of subjective probability and has applications in areas such as actuarial science, statistical inference, and machine learning.

\subsection{Intuition}
The intuitive idea behind De Finetti's Theorem is that for exchangeable random variables, there's an underlying factor that, once known, renders the variables independent of one another. This can be likened to having a bag of colored balls where the color distribution is unknown. If we assume that drawing any ball does not affect the next (exchangeability), De Finetti tells us that there's some unknown distribution of colors, and each draw is like drawing from a bag with a specific color ratio.

\subsection{History}
The theorem is named after the Italian mathematician Bruno de Finetti, who introduced the concept in the early 20th century. De Finetti's work laid the groundwork for the modern Bayesian interpretation of probability, emphasizing the role of personal judgment and subjective assessment in statistical analysis. His theorem formalized the idea that probability is fundamentally a measure of belief rather than frequency, profoundly influencing statistical thought and practice.

\section{Law of Total Variance}

\subsection{Statement}
The Law of Total Variance, also known as the Law of Total Variation or Eve's Law, establishes that for a regression model involving a response variable \( y \in \mathbb{R} \) and a covariate vector \( x \in \mathbb{R}^p \), the marginal variance of \( y \) can be decomposed into the sum of the expected conditional variance of \( y \) given \( x \) and the variance of the conditional expectation of \( y \) given \( x \):
\[ \text{Var}(y) = \text{E}_x[\text{Var}(y|x)] + \text{Var}_x[\text{E}(y|x)] \]

For homoscedastic cases where variance is constant, the conditional variance is \( \sigma^2 \), leading to a simplified variance equation. In heteroscedastic cases, the variance is a function of \( x \), and the decomposition reflects this variability.

\subsection{Importance and Use Cases}
This law is crucial for understanding the behavior of variance in complex models, particularly in regression analysis. It allows statisticians and data scientists to separate the variability explained by the model from the unexplained variability, aiding in model assessment and improvement.

\subsection{Intuition}
The Law of Total Variance tells us that the overall uncertainty in \( y \) (the total variance) comes from two sources: the average uncertainty after we know \( x \) (the expected conditional variance) and the uncertainty in our prediction of \( y \) from \( x \) (the variance of the expected value). If you know the covariates, you reduce some uncertainty about \( y \), but not all of it—there's still the inherent variability in the relationship between \( x \) and \( y \).

\subsection{History}
While the Law of Total Variance is a classical result deriving from the foundations of probability theory, it doesn't have a specific attribution to a single mathematician. Instead, it is part of the broader development of statistical theory related to expectation and variance, which involves the cumulative work of many contributors over the years.
\end{document}